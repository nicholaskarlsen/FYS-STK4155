\documentclass[reprint, english, nofootinbib]{revtex4-2}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{subfig}
\usepackage{blindtext}
\usepackage{tikzducks}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}

\graphicspath{{../figs/}}


\begin{document}
\title{Solving Differential Equations with Neural Networks}
\author{Nicholas Karlsen}
\affiliation{University of Oslo}
\author{Thore Espedal Moe}
\affiliation{University of Oslo}
\date{\today}

\begin{abstract}
    We investigate the application of neural networks to the solution of both partial differential equations, examplified by the 1-D heat equation; as well as systems of nonlinear ordinary differential equations, examplified by a system of equations describing the dynamics of a class of Recurrent Neural Networks that can be used to find eigenvalues and eigenvectors of a real, symmetric matrix. For the heat equation we compare the solution from a neural network with both the analytic solution and a numerical solution from a simple Finite-Difference scheme for different resolutions of the computational grid and different amounts of training points for the network. We perform a similar analysis for the nonlinear system; this time comparing the networks solution with a numerical integration using the Forward-Euler method. However, as the nonlinear system does not readily admit an analytical solution, only the equilibrium state of the system can be independently computed (through standard numerical eigendecomposition of the mentioned matrix) as a "ground truth" of the solution. In general we find that the neural network-based solution can reach a reasonable level of accuracy with fewer grid-points/training points than the standard numerical integration schemes, and with far less stringent requirements on the distance between grid-points. Notably, the neural network-solution does not need its training grid-points to satisfy the famously restrictive stability criterion for the Finite-Difference method applied to the heat equation. We do find, however, that it is more difficult to scale up the accuracy of the network-based solutions, as compared to more traditional schemes.
\end{abstract}

\maketitle

\section{Introduction}

Per the preponderating proliferation of differential equations in the sciences, both natural and otherwise, there is intense and inherent interest in novel ways of obtaining their solutions. Thus, one of the more exciting developments in machine learning, and the topic of this paper, is the application of neural networks to the task of solving differential equations. 

We will be looking at two distinct problem types; partial differential equations (PDE) and systems of nonlinear ordinary differential equations (ODE). Concretely, we will be using the 1-D heat equation as an example of a PDE, while a method from \cite{Yi_2004} for finding eigenvalues and eigenvectors of real and symmetric matrices will serve as the example of nonlinear systems of ODEs.

In both cases we will compare solutions obtained from neural network-based approaches with those from more traditional numerical methods, namely the simplest available Finite-Difference schemes. For the heat equation an analytic solution is easily obtained as the ground truth of the methods. For the eigenproblem-related system an analytic solution is not easily acquired; rather, as the steady-state equilibrium points of the system correspond to the eigenvectors of a given matrix, the asymptotic solution can be computed by standard numerical eigendecomposition of the matrix.

Structurally, we begin with a brief discussion of how neural networks can be applied to solve differential equations in general. We then move on to a more detailed description of the two particular use cases, starting with a short introduction to the context in which they arise, as well as some aspects of their analytically known properties. Continuing on we describe the traditional methods we use for solving the equations, before presenting the specific setup of the neural network-based methods.

Subsequently we compare the performance of the network-based methods with the traditional numerical integration-methods, paying special attention to the effects and requirements of the computational grids' sizes on the convergence of the methods. We find that, in general, the neural networks place far laxer demands on the computational grids for achieving reasonable convergence. Especially noteworthy, we find that the distance between training grid-points for the neural network in the PDE-case do not have to satisfy the same strict stability criterion as the Finite-Difference scheme. However, we do note that it is more difficult to increase the accuracy of the network-methods, as compared to the traditional schemes. In most regards, we conclude that the network-based methods are inferior to the standard methods for the toy problems at hand. However, for different/larger problem dimensions we recognize the possibility of the network-based methods to achieve superior performance in terms of computational cost for some levels of accuracy.

\section{Theory \& Methods}

\subsection{Solving Differential Equations with Neural Networks}
\noindent
In general, a ordinary differential equation (ODE) may be written in the form
\begin{equation}
    F \qty[x, u(x), u'(x), u''(x), \dots, g^{(n)}(x)] = 0
\end{equation}    
where $u(x)$ denotes the solution of the ODE. One may then propose a trial solution $\tilde u(x,P)$ in the form
\begin{equation}
    \tilde u(x, P) = u_{bc}(x) + u_{nn}(x)N(x, P) 
\end{equation}
where $u_{bc}(x), u_{nn}(x)$ are functions which enforce the boundary conditions of the ODE and $N(x, P)$ is a neural network with independent variables denoted by $P = \{W\},\{\pmb b\}$, the weights and biases in the network respectively.. In order to optimize $N(x, P)$ such that $\tilde u(x)$ yields a solution of the ODE, we define the cost function by the $L^2$ norm of $F$ as
\begin{equation}
    \mathcal C\qty[u\qty(x)] = \frac{1}{N} \sum_{i=1}^{N}\qty(F\qty[x, u(x_i), u'(x_i), \dots, u^{(n)}(x_i)])^2
\end{equation}
where we have discretized as $x \rightarrow \{x_1, x_2 ,\dots, x_{N}\}$. We can then construct a Feed-Forward Neural network with $L-1$ hidden layers which for each itteration updates the weights and biases of the network wrt. to the current trial solution $\tilde u(x)$ evaluated with the cost function. See \cite{4155_project_2}

As a note, for some network architectures it is possible to obatin fairly simple derivatives of these types of cost-functions with respect to the network parameters, see e.g. \cite{Lagaris_1998}. However, it is in general quite burdensome to set up the network-derivatives by hand. Thus a major boost for the use of neural networks in solving differential equations comes from the advent of Automatic Differentiation, which by tracking the elementary operations performed in the network computations is capable of accurately and automatically producing the sought-after gradients. In the present work we rely on the autograd-functionality of PyTorch, to produce all necessary derivatives.

\subsection{The Heat Equation}

As our first example we investigate the application of neural networks for solving the so-called heat equation. In physics, the heat equation models how the temperature of some material $u\qty(\pmb r, t)$ evolves over time by the partial differential equation (PDE)
\begin{equation}
    \pdv{u}{t} = c^2\nabla^2 u 
\end{equation}
where $c^2$, the thermal diffusivity is a material dependent constant. 
This may be reduced to a one dimensional problem, modelling i.e a thin, insulating wire of length $L$ as
\begin{equation}\label{eqn: 1D heat eqn}
    \pdv{u}{t} = \pdv[2]{u}{x}
\end{equation}
where we have also for simplicity set $c^2 = 1$. We may then solve this analytically with boundary conditions $u\qty(0, t) = u\qty(L,t) = 0$ and initial condition $u(x, 0) = f(x)$.

\subsubsection{Solving the Heat Equation analytically}
\noindent 

We start by making the assumption that $u(x, t)$ is separable, giving 
\begin{equation}
    u(x, t) = F(x) \cdot G(t)
\end{equation}
which lets us rewrite Eqn.~\ref{eqn: 1D heat eqn} to the form
\begin{equation}
    F \cdot \pdv{G}{t} = \pdv[2]{F}{x} \cdot G
\end{equation}
which may be expressed as
\begin{equation}
    \frac{\dot G}{G} = \frac{F^{''}}{F} = k
\end{equation}
where $k$ is some constant. We further have that the boundary conditions may only be satisfied for $k <  0$, so we set $k = -\rho^2$ and write
\begin{equation}
    F^{''} + \rho F = 0 \quad \quad \dot G + \rho^2G = 0
\end{equation}
starting with the spatial equation, we have a general solution in the form 
\begin{equation}
    F(x) = c_1 \cos(\rho x) + c_2 \sin(\rho x)
\end{equation}
where $c_1 = 0$ is fixed by $u(0, t) = $. Similarly, $u(L, t) = 0$ imposes the requirement $\sin(\rho x) = 0$ which is satisfied by setting $\rho = n\pi/L$, yielding a discrete spectrum of solutions
\begin{equation}
    F_n(x) = \sin\qty(\frac{n\pi x}{L})
\end{equation}
where we omit the constant $c_1$, which we will include later in $G(t)$. 

We then move on to the temporal part, $G(t)$, which we write as
\begin{equation}
    \dot G = -\rho^2G
\end{equation}
which we immediately recognize as having a general solution
\begin{equation}
    G(t) = c_n e^{-n^2\pi^2 t / L^2}
\end{equation}
Thus, $u(x, t)$ is solved by a superposition of the discrete set of eigenfunctions
\begin{equation}
    u_n\qty(x, t) = \sum_{n=1}^{\infty} c_n \sin\qty(\frac{n\pi x}{L}) e^{-n^2\pi^2 t / L^2}
\end{equation}
with initial condition
\begin{equation}
    u(x, 0) = \sum_{n=1}^{\infty} c_n \sin\qty(\frac{n\pi x}{L}) = f(x)
\end{equation}
a Fourier series. Thus, the coefficients $c_n$ are determined by the integral
\begin{equation}
    c_n = \frac{2}{L}\int_0^L \dd x \, f(x) \sin\qty(\frac{n\pi x}{L})
\end{equation}
if we set the initial condition $f(x) = \sin(\pi x)$ and fix $L = 1$, we get coefficients
\begin{equation}
    c_n = 2\int_0^1 \dd x \, \sin(\pi x) \sin(n\pi x) = 1
\end{equation}
we thus have an analytical solution for the heat equation with boundary conditions $u(0, t) = u(L, t) = 0$, $L = 1$, and initial condition $u(x, 0) = \sin(\pi x)$ as
\begin{equation}
    u(x, t) = \sin\qty(\pi x) e^{-\pi^2 t}
\end{equation}
a plot of which for $t \in [0, 1]$ is shown in Fig.~\ref{fig: heat eqn analytic}.
\begin{figure}[h!tb]
    \center
    \includegraphics[width=.8\columnwidth]{heat_eqn_analytic.pdf}
    \caption{\label{fig: heat eqn analytic}}
\end{figure}

\subsubsection{Solving the Heat Equation through a simple Finite-Difference scheme}

While we in this case were able to obtain a simple analytic solution to the differential equation, that is generally not possible for most PDEs so numerical methods must be used instead. The simplest, though neither the most accurate nor the most stable, class of numerical methods for solving PDEs are the so-called explicit finite-difference schemes. The main idea consists of dividing the computational domain into a set of grid points, and discretizing the differential equations to obtain relations between the function values at those grid-points. 

The distinction between explicit and implicit schemes is, of course, that with an explicit scheme the values at the next grid points can be explicitly calculated from the previous grid points so that given the boundary conditions one obtains a rule for calculating the function values of the whole domain one step at the time. Oppositely, implicit schemes give the value at a grid-point as a function of both that grid-point's value and the values of the previous grid-points thus requiring the (usually numerical) solution of a linear system of equations for each grid-point. The benefit of implicit methods is that they usually have more lax restrictions on how the grid-points must be spaced in order to maintain stability of the solution.

For the case at hand one rewrites %% u(x_i,t_i) = (u(x_i-1,t_i-1) - 2*u(x_i,t_i-1) + u(x_i+1,t_i-1) * dt/dx2. First setup grid, then discretize du/dx2 into (u(x_i-1)-2*u(x_i) + u(x_i+1))/dx2 and du/dt = (u(t_i) - u(t_i-1))/dt. Move around these two equations and you get the formula. Can be rewritten as a matrix equation: u_vector(t_i+1) = tridiagonal matrix * u_vector(t_i) * dt/dx2 with zero-padding on top and bottom row to accomodate the BC. Stability criterion dt/dx2 <= 1/2, if not fulfilled round-off errors will cause the solution to explode. 

\begin{equation} \label{eqn: Az = b}
    B = \qty[
    \begin{matrix}
    0 \\
    1 & -2 & 1 \\
    & 1 & -2 & 1 \\
    && \ddots & \ddots & \ddots \\
    &&& 1 & -2 & 1 \\
    &&&&& 0
    \end{matrix}
    ]
 \end{equation}

\subsubsection{Solving the Heat Equation with Neural Networks}
\noindent
We now turn to the use of a neural network as an alternative method for obtaining the solution of the heat equation. We start by re-writing equation.~\ref{eqn: 1D heat eqn} to the form
\begin{equation}
    \pdv{u}{t} - \pdv[2]{u}{x} = 0
\end{equation}
which gives rise to the cost function 
\begin{equation}
    \mathcal C\qty[u\qty(x, y)] = \frac{1}{N}\sum_{i=1}^N \qty(\pdv{u}{t} - \pdv[2]{u}{x})^2
\end{equation}
setting the boundary conditions $u(0, t) = u(L, t) = 0$ and initial condition $u(x, 0) = \sin(\pi x)$, we then have a trial solution in the form
\begin{equation}
    u_t(x, t, P) = \qty(1 - t)\sin(\pi x) + x(1 - x)t N(x, t, P)
\end{equation}

\begin{figure}[h!tb]
   \center
   \input{../figs/NN.tex} 
   \caption{Possible network structure of a Feed-Forward network with input $(x,t)$ and 2 hidden layers each with 3 neurons with activations denoted as $a^l_j$ and a single output $N$.}
\end{figure}


\subsection{A Nonlinear System of Ordinary Differential Equations for Finding Eigenvalues and Eigenvectors of Real Symmetric Matrices}

For our second example, we investigate the use of neural networks for the solution of a nonlinear system of ordinary differential equations. The particular use case is the system of differential equations presented by \cite{Yi_2004} for finding the largest eigenvalue $\lambda_{1}$ and corresponding eigenvector $\mathbf{v}_1$ of an $N \times N$ real and symmetric matrix $\mathbf{A}$:

\begin{equation}
\label{eq:yi_eq}
\frac{d \mathbf{x}(t)}{dt} = - \mathbf{x}(t) + [\mathbf{x}(t)^T \mathbf{x} \mathbf{A} - (1 - \mathbf{x}(t)^T \mathbf{x}(t)) \mathbf{I} ] \mathbf{x}(t)
\end{equation}

where $\mathbf{x}(t)$ is a $ 1 \times N$ column vector and $\mathbf{I}$ is an $N \times N$ identity matrix. In \cite{Yi_2004} it is shown that for any initial vector $\mathbf{x}(0)$ which is not orthogonal to eigenspace of the eigenvalue $\lambda_1$, the solution $\mathbf{x}(t)$ of the system \ref{eq:yi_eq} as $t$ goes to infinity converges to an eigenvector $\mathbf{v}_1$ corresponding to the largest eigenvalue of $\mathbf{A}$. It is further shown in \cite{Yi_2004} that replacing $\mathbf{A}$ with $\mathbf{-A}$ in \ref{eq:yi_eq} makes the solution converge instead to an eigenvector of the smallest eigenvalue $\lambda_N$ belonging to $\mathbf{A}$, so long as the initial vector $\mathbf{x}(0)$ is not orthogonal to the eigenspace of that eigenvalue $\lambda_N$. Finally, they give the following recipe for obtaining the eigenvalue from the converged eigenvector:

\begin{equation}
\label{eq:yi_eigenvalue}
\lambda = \frac{\mathbf{v}^T \mathbf{A} \mathbf{v}}{\mathbf{v}^T \mathbf{v}}
\end{equation}
which follows immediately from the definition of eigenvectors and eigenvalues.

Thus, given a real and symmetric matrix, the largest and smallest eigenvalues along with their corresponding eigenvectors can easily obtained from solving \ref{eq:yi_eq} with an appropriate initial vector $\mathbf{x}(0)$. Now, in order to obtain the remaining eigenvalues and eigenvectors one could in principle apply a deflation technique, like Hotelling's or Wielandt's deflation \cite{Saad_2011} \cite{Roberts}, to the matrix $\mathbf{A}$ and repeat the process for the deflated matrix $\mathbf{A}_D$. That is, however, beyond the scope of our present work; and furthermore, non-trivial numerical issues might be expected to arise in such repeated solutions of \ref{eq:yi_eq}.

\subsubsection{Solution through the Forward-Euler Method}

The authors of \cite{Yi_2004} presented the equation \ref{eq:yi_eq} in the context of Recurrent Neural Networks (RNN), where it describes the dynamics of a certain class of such networks. RNNs are outside our current areas of expertise; however, to the best of our knowledge, the simplest RNN that obeys the dynamics prescribed by \ref{eq:yi_eq} should be completely equivalent to the following explicit integration method (Forward-Euler):

\begin{equation}
\label{eq:yi_euler}
\mathbf{x}(t_{i+1}) = \mathbf{x}(t_i) + \frac{d \mathbf{x}(t_i)}{dt}  dt
\end{equation}
where $dt = t_{i+1} -t_{i}$ is a suitably small time-step,  $\frac{d \mathbf{x}(t_i)}{dt}$ is equation \ref{eq:yi_eq} evaluated at $t=t_i$, and the iterations are started from some initial vector $\mathbf{x}(0)$.

This is, in any case, the simplest numerical integration scheme for solving equation \ref{eq:yi_eq}, and is what we will be using to compare our neural network-based solutions with.

\subsubsection{Solution through Neural Networks}

The procedure for solving equation \ref{eq:yi_eq} with a neural network is very much the same as for the heat equation. First a trial solution satisfying the initial conditions is constructed, in this case we use:

%% TRIAL FUNCTION

Then the cost-function to be optimized is defined, in this case as:

%% COST FUNCTION

Finally the derivatives of the cost-function with respect to the network parameters are computed, and the network parameters are updated:

%% UPDATE

The only real change from the case of the heat equation is that the network now must produce $N$ outputs for each input $t$, as opposed to one output for each pair $x,t$ of inputs.


\section{Results \& Discussion}

\subsection{The Heat Equation}

\begin{figure*}[h!tb]
    \center
    \subfloat[Model I evaluated on 11x11 grid]{
    \includegraphics[width=.66\columnwidth]{PDE_RELU_10_10x1000.pdf}
    }
    \subfloat[Model I evaluated on 100x100 grid]{
    \includegraphics[width=.66\columnwidth]{PDE_RELU_10_100x100.pdf}
    }
    \subfloat[Error of model I]{
    \includegraphics[width=.66\columnwidth]{PDE_RELU_10_ERR.pdf}
    }
    \\
    \subfloat[Model II evaluated on 11x11 grid]{
    \includegraphics[width=.66\columnwidth]{PDE_RELU_100_10x10.pdf}
    }
    \subfloat[Model II evaluated on 100x100 grid]{
    \includegraphics[width=.66\columnwidth]{PDE_RELU_100_100x100.pdf}
    }
    \subfloat[Error of model II]{
    \includegraphics[width=.66\columnwidth]{PDE_RELU_100_ERR.pdf}
    }
    \caption{\label{fig:ReLU composite figure}Feed-Forward Neural Networks with 4 hidden layers each consisting of 100 Neurons using the ReLU activation function trained on $11\times11$ (Model I) and $100\times100$ (Model II) points in the intervals $[x,t]\in[0,1]\times[0,0.3]$}, where absolute error, and mean absolute error (MAE) are computed with respect to the analytic solution of the PDE on a grid of $200\times200$ points.
\end{figure*}

\begin{figure}[h!tb]
    \subfloat[$\Delta x = 1/10,\enspace \Delta t =  1/200$]{
    \includegraphics[width=.5\columnwidth]{heat_eq_euler_err_10.pdf} 
    }
    \subfloat[$\Delta x = 1/100,\enspace \Delta t = 1/20000$]{
    \includegraphics[width=.5\columnwidth]{heat_eq_euler_err_100.pdf} 
    }
    \caption{\label{fig:fw euler PDE error}Absolute error of the heat equation solved using an explicit forward euler scheme wrt. the analytic solution for $\Delta x = 1/10$ and $\Delta x = 1/100$}
\end{figure}

\subsection{The Eigen-problem}

\begin{equation}
    A = \qty(
    \begin{matrix}
     -0.0 & 0.7 & -2.4 & 0.6 & 0.3 & -3 \\
     0.7 & -3.4 & 1.2 & -0.2 & -1.4 & -1 \\
     -2.4 & 1.2 & -1.7 & -0.4 & -1.1 & -0 \\
     0.6 & -0.2 & -0.4 & 1.7 & 2.1 & -1 \\
     0.3 & -1.4 & -1.1 & 2.1 & -3.5 & 1 \\
     -2.8 & -1.0 & -0.4 & -0.7 & 0.5 & -2 \\
    \end{matrix}
    )
\end{equation}
\begin{figure}[h!tb]
   \center
   \includegraphics[width=.8\columnwidth]{eigenvec_dynamics.pdf} 
   \caption{\label{fig:eigenvector} Dynamics of the eigenvector components of the largest eigenvalue of the matrix $A$ modelled by the Neural network (Solid lines) as well as the same dynamics solved using a Forward Euler method (dotted lines) where ground truth is indicated by the semi-transparrent dashed lines.}
\end{figure}
\section{Conclusion}

\onecolumngrid
\bibliography{bibfile}
\twocolumngrid

\end{document}