{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import hessian, jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(N, M):\n",
    "    \"\"\"Splits data set x into M roughly equally minibatches. If not evenly divisible, the excess\n",
    "    is evenly spread throughout some of the batches.\n",
    "\n",
    "    Args:\n",
    "        N (Int): Number of datapoints\n",
    "        M (Int): Number of minibatches\n",
    "\n",
    "    Returns:\n",
    "        Array: [M,.]-dim array containing the minibatch indices\n",
    "    \"\"\"\n",
    "    indices = np.random.permutation(N)  # random permutation of [0, ..., len(x)-1]\n",
    "    indices = np.array_split(indices, M)  # Split permutation into M sub-arrays\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 100\n",
    "\n",
    "x = np.linspace(0,1,grid_size, dtype=np.float)\n",
    "t = np.linspace(0,1,grid_size, dtype=np.float)\n",
    "x, t = np.meshgrid(x, t) \n",
    "x = x.flatten()\n",
    "t = t.flatten()\n",
    "X = np.concatenate((x.reshape(-1,1), t.reshape(-1, 1)), axis=1)\n",
    "\n",
    "num_points, input_dim = X.shape\n",
    "hidden_neurons = 100\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9899, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<IndexBackward>)\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0101, 0.0000],\n",
      "        [0.0202, 0.0000],\n",
      "        ...,\n",
      "        [0.9798, 1.0000],\n",
      "        [0.9899, 1.0000],\n",
      "        [1.0000, 1.0000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mb = minibatch(num_points, num_points / 32)\n",
    "\n",
    "print(x[[num_points-2, num_points-1]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_trial(x, t, N):\n",
    "    return (1 - t) * torch.sin(np.pi * x) + x * (1 - x) * t * N\n",
    "\n",
    "#@torch.jit.script\n",
    "def ode_loss(input_data, output_data):\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(input_data.size(0)):\n",
    "        Jx, Jt, Jn = jacobian(g_trial, (input_data[i,0], input_data[i,1], output_data[i]),create_graph=True)\n",
    "        Hx, Ht, Hn = hessian(g_trial, (input_data[i,0], input_data[i,1], output_data[i]),create_graph=True)\n",
    "        loss = loss + (Jt - Hx[0]).pow(2)\n",
    "    \n",
    "    return loss / input_data.size(0)\n",
    "\n",
    "#ode_loss_jit = torch.jit.script(ode_loss)\n",
    "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.functional.hessian\n",
    "#u_hess = hessian(g_trial, (x, N))\n",
    "# https://pytorch.org/docs/stable/autograd.html#torch.autograd.functional.hessian\n",
    "#u_jacob = jacobian(g_trial, (x, N))\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.from_numpy(X)\n",
    "x = x.to(dtype).to(device)\n",
    "x.requires_grad = True\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(input_dim, hidden_neurons, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(hidden_neurons, output_dim, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "learning_rate = 0.002\n",
    "N_minibatches = int(num_points / 32)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(input_dim, hidden_neurons),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(hidden_neurons, output_dim),\n",
    "        )\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss =  tensor([10.1659], grad_fn=<DivBackward0>)\n",
      "1: loss =  tensor([10.0906], grad_fn=<DivBackward0>)\n",
      "2: loss =  tensor([10.0250], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-21566a0c10aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mN_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Compute and print loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mode_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Use autograd to compute the backward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-877425525dc1>\u001b[0m in \u001b[0;36mode_loss\u001b[0;34m(input_data, output_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mJx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mHx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mJt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mHx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tuple_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m     is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    427\u001b[0m                                           \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjac_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jacobian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m     is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    427\u001b[0m                                           \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mis_out_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hessian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-877425525dc1>\u001b[0m in \u001b[0;36mg_trial\u001b[0;34m(x, t, N)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mg_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#@torch.jit.script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mode_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mrelevant_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__rsub__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mb = minibatch(num_points, N_minibatches)\n",
    "    for i in range(N_minibatches):\n",
    "        N_output = model(x[mb[i]])\n",
    "        # Compute and print loss\n",
    "        loss = ode_loss(x[mb[i]], N_output)\n",
    "        print(f\"{epoch}: loss = \",loss) \n",
    "        # Use autograd to compute the backward pass.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Compute loss of entire sample\n",
    "    #N_output = model(x) \n",
    "    #loss = ode_loss(x, N_output)\n",
    "    #print(f\"{epoch}: loss = \",loss) \n",
    "        \n",
    "def g_trial_np(x, t, N):\n",
    "    return (1 - t) * np.sin(np.pi * x) + x * (1 - x) * t * N\n",
    "\n",
    "xp = np.linspace(0,1,grid_size, dtype=np.float)\n",
    "tp = np.linspace(0,1,grid_size, dtype=np.float)\n",
    "xp, tp = np.meshgrid(xp, xp) \n",
    "\n",
    "N_pred = model(x)\n",
    "N_pred = N_pred.detach().numpy()\n",
    "N_pred = N_pred.reshape(xp.shape)\n",
    "\n",
    "g = g_trial_np(xp, tp, N_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "cf = ax.pcolormesh(xp, tp, g, cmap=plt.get_cmap(\"inferno\"))\n",
    "fig.colorbar(cf, ax=ax)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$t$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss =  tensor([0.0529], grad_fn=<DivBackward0>)\n",
      "100: loss =  tensor([0.0528], grad_fn=<DivBackward0>)\n",
      "200: loss =  tensor([0.0532], grad_fn=<DivBackward0>)\n",
      "300: loss =  tensor([0.0516], grad_fn=<DivBackward0>)\n",
      "400: loss =  tensor([0.0513], grad_fn=<DivBackward0>)\n",
      "500: loss =  tensor([0.0512], grad_fn=<DivBackward0>)\n",
      "600: loss =  tensor([0.0526], grad_fn=<DivBackward0>)\n",
      "700: loss =  tensor([0.0501], grad_fn=<DivBackward0>)\n",
      "800: loss =  tensor([0.0494], grad_fn=<DivBackward0>)\n",
      "900: loss =  tensor([0.0491], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mb = minibatch(num_points, N_minibatches)\n",
    "    for i in range(N_minibatches):\n",
    "        N_output = model(x[mb[i]])\n",
    "        # Compute and print loss\n",
    "        loss = ode_loss(x[mb[i]], N_output)\n",
    "        # Use autograd to compute the backward pass.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Compute loss of entire sample\n",
    "    if epoch % 100 == 0:   \n",
    "        N_output = model(x) \n",
    "        loss = ode_loss(x, N_output)\n",
    "        print(f\"{epoch}: loss = \",loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-16f7f0e46d5e>:15: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  cf = ax.pcolormesh(xp, tp, g, cmap=plt.get_cmap(\"inferno\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$t$')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def g_trial_np(x, t, N):\n",
    "    return (1 - t) * np.sin(np.pi * x) + x * (1 - x) * t * N\n",
    "\n",
    "xp = np.linspace(0,1,grid_size, dtype=np.float)\n",
    "tp = np.linspace(0,1,grid_size, dtype=np.float)\n",
    "xp, tp = np.meshgrid(xp, xp) \n",
    "\n",
    "N_pred = model(x)\n",
    "N_pred = N_pred.detach().numpy()\n",
    "N_pred = N_pred.reshape(xp.shape)\n",
    "\n",
    "g = g_trial_np(xp, tp, N_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "cf = ax.pcolormesh(xp, tp, g, cmap=plt.get_cmap(\"inferno\"))\n",
    "fig.colorbar(cf, ax=ax)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$t$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, t, N):\n",
    "    return (x + t - N).mean()\n",
    "\n",
    "x_test, y_test, N_test = (x[:, 0].reshape(-1,1), x[:, 1].reshape(-1,1), N_output)\n",
    "dx, dt, dN = jacobian(f, (x[:, 0].reshape(-1,1), x[:, 1].reshape(-1,1), N_output))\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dt, dN = hessian(f, (x[:, 0].reshape(-1, 1), x[:, 1].reshape(-1, 1), N_output))\n",
    "#print(dx[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dt, dN = torch.autograd.grad([f(x_test, y_test, N_test)], [x_test, y_test, N_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.linspace(0,1, 10, requires_grad=True)\n",
    "x2 = torch.linspace(0,1, 10, requires_grad=True)\n",
    "z = torch.sin(x1) * torch.cos(x2)\n",
    "\n",
    "print(z)\n",
    "torch.autograd.grad(z, [x1, x2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
