{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad, elementwise_grad, hessian, jacobian\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../../project_2/src\")\n",
    "from SGD import minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by mortens example code in week 43\n",
    "\n",
    "def activation_function(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def activation_out(z):\n",
    "    return z\n",
    "\n",
    "\"\"\"\n",
    "network_shape = [inputs, w1, w2, ..., wL, outputs]\n",
    "\n",
    "W = wwwwb\n",
    "    wwwwb\n",
    "\"\"\"\n",
    "def initialize_params(network_shape):\n",
    "    P = []\n",
    "    for i in range(1, len(network_shape)):\n",
    "        k = network_shape[i-1]\n",
    "        j = network_shape[i]\n",
    "        P.append(np.random.randn(j, k) * np.sqrt(2) / np.sqrt(k))\n",
    "        P[i-1] = np.concatenate((P[i-1], np.zeros(j).reshape(-1,1)), axis=1)\n",
    "\n",
    "    return P\n",
    "\n",
    "def Network(t, P):\n",
    "    # Assumes the input t to be a scalar, returns a 2d-row vector e.g. shape=[1,6]\n",
    "    a = t.reshape(-1,1)\n",
    "    for P_i in P:\n",
    "        #a = np.concatenate((a, np.ones(np.size(a, 0)).reshape(-1, 1)), axis=1) \n",
    "        a = np.concatenate((a, np.ones((1,1))), axis=1) \n",
    "        z = np.matmul(a, np.transpose(P_i))\n",
    "        a = activation_function(z)\n",
    "    \n",
    "    return activation_out(z) \n",
    "\n",
    "def Network_predict(t, P):\n",
    "    # For predictions\n",
    "    # Assumes the input t to be a 1d-array, \n",
    "    # returns a matrix where each row corresponds to a vector for a particular t\n",
    "    a = t.reshape(-1,1)\n",
    "    for P_i in P:\n",
    "        a = np.concatenate((a, np.ones(np.size(a, 0)).reshape(-1, 1)), axis=1) \n",
    "        #a = np.concatenate((a, np.ones((1,1))), axis=1) \n",
    "        z = np.matmul(a, np.transpose(P_i))\n",
    "        a = activation_function(z)\n",
    "    \n",
    "    return activation_out(z) \n",
    "\n",
    "def optimize(t, P, A, x_0, N_minibatches, learning_rate, n_epochs):\n",
    "    # Assumes t is a 1d-array.\n",
    "    assert N_minibatches <= np.size(t, 0)\n",
    "\n",
    "    cost_func_grad = grad(costfunction, 1) # Check which grad-call is correct.\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        mb = minibatch(t, N_minibatches)\n",
    "        for i in range(N_minibatches):\n",
    "            t_mb = t[mb[i]]\n",
    "            M = np.size(t_mb, 0)\n",
    "            # compute gradients of weights\n",
    "            cost_grad = cost_func_grad(t_mb, P, A, x_0)\n",
    "            for l in range(len(P)):\n",
    "                P[l] -= learning_rate * cost_grad[l]\n",
    "    return P\n",
    "\n",
    "def g_trial(t, P, x_0):\n",
    "    #assumes the input t to be a scalar, x_0 is 1d-row-vector which broadcasts to e.g. (1,6)\n",
    "    \n",
    "    return np.exp(-t)*x_0 + (1-np.exp(-t))*Network(t,P)\n",
    "\n",
    "def g_trial_predict(t, P, x_0):\n",
    "    # For predictions\n",
    "    #assumes the input t to be a 1d-array, broadcasts along the rows of Network_predict's output\n",
    "    t = t.reshape(-1,1)\n",
    "    return np.exp(-t)*x_0 + (1-np.exp(-t))*Network_predict(t,P)\n",
    "\n",
    "def costfunction(t, P, A, x_0):\n",
    "    \n",
    "    cost = 0\n",
    "    g_grad = jacobian(g_trial,0) # Check that this is the correct grad-call\n",
    "    \n",
    "    for time in t:\n",
    "        d_dt = g_grad(time,P,x_0).reshape(-1,1) # should have shape (eigenvector_length,1)\n",
    "        x_t = g_trial(time,P,x_0).reshape(-1,1) #check shape, should be (eigenvector_length,1)\n",
    "        # Right hand side is: -x + [x.T @ x @ A + (1 - x.T @ A @ x) @ I] @ x where x is x_t is a column vector\n",
    "        right_side = -x_t + np.matmul( # -x + [\n",
    "                                np.matmul(np.transpose(x_t), np.matmul(A, x_t)) #x.T @ A @ x\n",
    "                                + ( 1 - np.matmul(np.transpose(x_t),np.matmul(A, x_t))) # + (1-x.t @ A @ x)\n",
    "                                * np.identity(np.size(x_t, 0)) # * I\n",
    "                                , x_t) # ] @ x\n",
    "        cost = cost + np.sum((d_dt - right_side)**2) / np.size(x_t) \n",
    "                                                              \n",
    "        \n",
    "\n",
    "    return cost / np.size(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,1,20)\n",
    "Q = np.random.randn(6,6)\n",
    "A = (np.transpose(Q) + Q) /2 #standard trick for making symmetric real\n",
    "x_0 = np.random.randn(6) #initial guess for eigenvector.\n",
    "\n",
    "network_shape = [1, 10, np.size(x_0)] #output must match eigenvector length\n",
    "P = initialize_params(network_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = optimize(t, P, A, x_0,10, 0.002, 10)\n",
    "#y = Network(X[0, :].reshape(1, 2), P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37038099, -0.03053606, -0.63589588, -0.1285855 ,  0.50985643,\n",
       "        -0.46647811],\n",
       "       [ 0.34960479, -0.0322334 , -0.60725409, -0.1236994 ,  0.48766743,\n",
       "        -0.44635143],\n",
       "       [ 0.32604887, -0.03221664, -0.57528048, -0.12132387,  0.46617283,\n",
       "        -0.42410407],\n",
       "       [ 0.29992456, -0.02936822, -0.53759712, -0.12114634,  0.44699049,\n",
       "        -0.39792149],\n",
       "       [ 0.27148759, -0.02430781, -0.49550677, -0.12301638,  0.4295234 ,\n",
       "        -0.36877857],\n",
       "       [ 0.24093133, -0.01720412, -0.44940244, -0.12679486,  0.41362508,\n",
       "        -0.33692905],\n",
       "       [ 0.2084465 , -0.00821474, -0.39963155, -0.13233796,  0.39919966,\n",
       "        -0.30260806],\n",
       "       [ 0.17420988,  0.00251351, -0.34651645, -0.13951155,  0.38615697,\n",
       "        -0.26603379],\n",
       "       [ 0.13838515,  0.01484392, -0.29035607, -0.14819066,  0.3744122 ,\n",
       "        -0.22740852],\n",
       "       [ 0.10112384,  0.02864922, -0.2314275 , -0.15825881,  0.36388561,\n",
       "        -0.18691973],\n",
       "       [ 0.06256612,  0.04381099, -0.16998739, -0.16960753,  0.35450225,\n",
       "        -0.14474104],\n",
       "       [ 0.02284153,  0.06021906, -0.10627337, -0.18213581,  0.34619161,\n",
       "        -0.1010332 ],\n",
       "       [-0.01793024,  0.077771  , -0.04050531, -0.19574963,  0.33888743,\n",
       "        -0.0559449 ],\n",
       "       [-0.0596387 ,  0.09637162,  0.02711351, -0.21036153,  0.33252743,\n",
       "        -0.00961359],\n",
       "       [-0.10218196,  0.11593246,  0.09639523, -0.22589013,  0.32705306,\n",
       "         0.03783371],\n",
       "       [-0.14546609,  0.13637139,  0.16716641, -0.24225981,  0.32240928,\n",
       "         0.08627977],\n",
       "       [-0.18940462,  0.15761217,  0.23926702, -0.25940026,  0.3185444 ,\n",
       "         0.13561644],\n",
       "       [-0.23391799,  0.17958408,  0.3125495 , -0.27724619,  0.31540981,\n",
       "         0.18574401],\n",
       "       [-0.27912868,  0.20185131,  0.38721316, -0.29551825,  0.31254904,\n",
       "         0.23688422],\n",
       "       [-0.32483166,  0.22461407,  0.46289663, -0.31431435,  0.31020878,\n",
       "         0.28873158]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_trial_predict(t,P,x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.43535591 -0.86674269  0.54381493  0.64100287  1.23398329  2.28431894]\n",
      "[[-0.39857791 -0.484146   -0.05187932  0.37540123  0.68051778  0.00403822]\n",
      " [-0.36878851  0.00343064  0.34339448 -0.4610389   0.07126209 -0.72693076]\n",
      " [-0.69474421 -0.14977827  0.25445235 -0.0401098  -0.47461927  0.45086439]\n",
      " [ 0.15702803  0.25173375  0.62447677 -0.29661133  0.47961362  0.45165648]\n",
      " [ 0.00801069  0.32019802  0.50963524  0.74607829 -0.13874704 -0.24859087]\n",
      " [-0.44467874  0.75977674 -0.4061323   0.01723845  0.23931955  0.04985641]]\n"
     ]
    }
   ],
   "source": [
    "eigval, eigvec = np.linalg.eigh(A)\n",
    "print(eigval)\n",
    "print(eigvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37038099 -0.03053606 -0.63589588 -0.1285855   0.50985643 -0.46647811]\n",
      "[[ 0.23484955]\n",
      " [-0.09377635]\n",
      " [-0.589981  ]\n",
      " [-0.17414605]\n",
      " [ 0.34917553]\n",
      " [-0.45111171]]\n",
      "[[ 0.45837711]\n",
      " [ 0.05322078]\n",
      " [-0.55854008]\n",
      " [-0.04586543]\n",
      " [ 0.59932735]\n",
      " [-0.3873309 ]]\n",
      "[[ 0.24349344]\n",
      " [ 0.09501306]\n",
      " [-0.12918307]\n",
      " [ 0.05870027]\n",
      " [ 0.29514842]\n",
      " [-0.06643888]]\n",
      "[[ 0.29039866]\n",
      " [ 0.16941991]\n",
      " [-0.01325048]\n",
      " [ 0.139833  ]\n",
      " [ 0.33248608]\n",
      " [ 0.03787219]]\n",
      "[[0.38258108]\n",
      " [0.27833818]\n",
      " [0.12093788]\n",
      " [0.25284423]\n",
      " [0.41884625]\n",
      " [0.16498839]]\n",
      "[[0.32711584]\n",
      " [0.21819489]\n",
      " [0.05373103]\n",
      " [0.19155687]\n",
      " [0.36500846]\n",
      " [0.09975837]]\n",
      "[[0.38652989]\n",
      " [0.28458059]\n",
      " [0.13064349]\n",
      " [0.25964757]\n",
      " [0.42199713]\n",
      " [0.17372478]]\n",
      "[[0.30159963]\n",
      " [0.19283574]\n",
      " [0.02860903]\n",
      " [0.16623613]\n",
      " [0.33943761]\n",
      " [0.07457   ]]\n",
      "[[0.37230116]\n",
      " [0.27312923]\n",
      " [0.1233858 ]\n",
      " [0.24887546]\n",
      " [0.40680217]\n",
      " [0.16529344]]\n",
      "[[0.30399011]\n",
      " [0.19925382]\n",
      " [0.04110854]\n",
      " [0.17363921]\n",
      " [0.34042692]\n",
      " [0.08536754]]\n",
      "[[0.36271484]\n",
      " [0.26530249]\n",
      " [0.11821589]\n",
      " [0.24147904]\n",
      " [0.39660372]\n",
      " [0.15937998]]\n",
      "[[0.30532118]\n",
      " [0.20317475]\n",
      " [0.04894   ]\n",
      " [0.17819353]\n",
      " [0.340857  ]\n",
      " [0.09210459]]\n",
      "[[0.35522395]\n",
      " [0.25898226]\n",
      " [0.11366331]\n",
      " [0.23544512]\n",
      " [0.38870556]\n",
      " [0.1543327 ]]\n",
      "[[0.30710338]\n",
      " [0.20683851]\n",
      " [0.0554448 ]\n",
      " [0.18231745]\n",
      " [0.34198462]\n",
      " [0.09781429]]\n",
      "[[0.34940872]\n",
      " [0.25394003]\n",
      " [0.10978825]\n",
      " [0.23059193]\n",
      " [0.38262141]\n",
      " [0.15013099]]\n",
      "[[0.30888486]\n",
      " [0.2099836 ]\n",
      " [0.06064884]\n",
      " [0.18579602]\n",
      " [0.34329171]\n",
      " [0.1024421 ]]\n",
      "[[0.34482187]\n",
      " [0.24986028]\n",
      " [0.10647417]\n",
      " [0.22663619]\n",
      " [0.37785816]\n",
      " [0.14660263]]\n",
      "[[0.3105837 ]\n",
      " [0.2126862 ]\n",
      " [0.06486706]\n",
      " [0.1887441 ]\n",
      " [0.34464136]\n",
      " [0.10623616]]\n",
      "[[0.34115674]\n",
      " [0.24652255]\n",
      " [0.10363081]\n",
      " [0.22337854]\n",
      " [0.37407912]\n",
      " [0.14362091]]\n",
      "[[0.31216046]\n",
      " [0.21501164]\n",
      " [0.06832297]\n",
      " [0.19125264]\n",
      " [0.34595765]\n",
      " [0.10937569]]\n",
      "[[0.33819736]\n",
      " [0.24376862]\n",
      " [0.1011871 ]\n",
      " [0.22067486]\n",
      " [0.37104827]\n",
      " [0.14109038]]\n",
      "[[0.31359769]\n",
      " [0.21701365]\n",
      " [0.07117775]\n",
      " [0.19339277]\n",
      " [0.34719841]\n",
      " [0.11199181]]\n",
      "[[0.3357874 ]\n",
      " [0.24148145]\n",
      " [0.09908534]\n",
      " [0.21841772]\n",
      " [0.36859559]\n",
      " [0.13893673]]\n",
      "[[0.31489086]\n",
      " [0.21873715]\n",
      " [0.07355103]\n",
      " [0.19522152]\n",
      " [0.34834187]\n",
      " [0.11418324]]\n",
      "[[0.333811  ]\n",
      " [0.23957221]\n",
      " [0.0972775 ]\n",
      " [0.2165249 ]\n",
      " [0.36659582]\n",
      " [0.13710051]]\n",
      "[[0.31604311]\n",
      " [0.2202203 ]\n",
      " [0.07553382]\n",
      " [0.1967856 ]\n",
      " [0.34937901]\n",
      " [0.1160262 ]]\n",
      "[[0.3321806 ]\n",
      " [0.23797202]\n",
      " [0.09572291]\n",
      " [0.21493209]\n",
      " [0.36495492]\n",
      " [0.13553316]]\n",
      "[[0.31706206]\n",
      " [0.22149586]\n",
      " [0.07719684]\n",
      " [0.19812391]\n",
      " [0.35030868]\n",
      " [0.11758078]]\n",
      "[[0.33082898]\n",
      " [0.23662656]\n",
      " [0.09438677]\n",
      " [0.21358814]\n",
      " [0.36360115]\n",
      " [0.13419442]]\n",
      "[[0.3179577 ]\n",
      " [0.2225921 ]\n",
      " [0.07859597]\n",
      " [0.19926921]\n",
      " [0.35113454]\n",
      " [0.11889515]]\n",
      "[[0.32970376]\n",
      " [0.23549239]\n",
      " [0.09323909]\n",
      " [0.21245179]\n",
      " [0.36247905]\n",
      " [0.13305051]]\n",
      "[[0.31874114]\n",
      " [0.22353354]\n",
      " [0.07977599]\n",
      " [0.2002493 ]\n",
      " [0.35186301]\n",
      " [0.1200084 ]]\n",
      "[[0.3287637 ]\n",
      " [0.23453436]\n",
      " [0.09225392]\n",
      " [0.21148936]\n",
      " [0.36154524]\n",
      " [0.13207294]]\n",
      "[[0.3194237 ]\n",
      " [0.22434146]\n",
      " [0.0807732 ]\n",
      " [0.20108787]\n",
      " [0.35250195]\n",
      " [0.12095263]]\n",
      "[[0.32797595]\n",
      " [0.23372376]\n",
      " [0.09140883]\n",
      " [0.21067317]\n",
      " [0.36076544]\n",
      " [0.1312375 ]]\n",
      "[[0.32001641]\n",
      " [0.22503432]\n",
      " [0.08161726]\n",
      " [0.20180522]\n",
      " [0.35305982]\n",
      " [0.12175438]]\n",
      "[[0.32731411]\n",
      " [0.23303697]\n",
      " [0.09068434]\n",
      " [0.20998027]\n",
      " [0.36011228]\n",
      " [0.13052356]]\n",
      "[[0.32052968]\n",
      " [0.22562811]\n",
      " [0.08233265]\n",
      " [0.20241871]\n",
      " [0.35354508]\n",
      " [0.12243573]]\n",
      "[[0.32675684]\n",
      " [0.23245441]\n",
      " [0.09006361]\n",
      " [0.20939154]\n",
      " [0.35956381]\n",
      " [0.12991352]]\n",
      "[[0.32097314]\n",
      " [0.22613671]\n",
      " [0.08293961]\n",
      " [0.20294324]\n",
      " [0.35396588]\n",
      " [0.12301517]]\n",
      "[[0.32628673]\n",
      " [0.23195983]\n",
      " [0.08953208]\n",
      " [0.20889097]\n",
      " [0.3591022 ]\n",
      " [0.12939232]]\n",
      "[[0.32135554]\n",
      " [0.22657212]\n",
      " [0.08345504]\n",
      " [0.20339161]\n",
      " [0.35432984]\n",
      " [0.12350821]]\n",
      "[[0.32588949]\n",
      " [0.2315396 ]\n",
      " [0.08907714]\n",
      " [0.20846512]\n",
      " [0.35871297]\n",
      " [0.1289471 ]]\n",
      "[[0.32168476]\n",
      " [0.22694468]\n",
      " [0.08389307]\n",
      " [0.20377478]\n",
      " [0.35464398]\n",
      " [0.12392791]]\n",
      "[[0.32555338]\n",
      " [0.23118233]\n",
      " [0.08868793]\n",
      " [0.20810268]\n",
      " [0.35838422]\n",
      " [0.12856683]]\n",
      "[[0.3219678 ]\n",
      " [0.22726335]\n",
      " [0.08426553]\n",
      " [0.20410216]\n",
      " [0.35491462]\n",
      " [0.12428532]]\n",
      "[[0.32526864]\n",
      " [0.23087843]\n",
      " [0.08835507]\n",
      " [0.20779408]\n",
      " [0.35810615]\n",
      " [0.12824208]]\n",
      "[[0.32221085]\n",
      " [0.22753582]\n",
      " [0.08458241]\n",
      " [0.20438182]\n",
      " [0.35514744]\n",
      " [0.12458977]]\n",
      "[[0.32502719]\n",
      " [0.2306198 ]\n",
      " [0.08807052]\n",
      " [0.20753126]\n",
      " [0.35787067]\n",
      " [0.12796478]]\n",
      "[[0.32241936]\n",
      " [0.22776871]\n",
      " [0.08485211]\n",
      " [0.20462067]\n",
      " [0.35534747]\n",
      " [0.12484917]]\n",
      "[[0.32482226]\n",
      " [0.23039962]\n",
      " [0.08782732]\n",
      " [0.20730735]\n",
      " [0.35767104]\n",
      " [0.12772802]]\n",
      "[[0.32259809]\n",
      " [0.22796771]\n",
      " [0.08508174]\n",
      " [0.20482464]\n",
      " [0.35551914]\n",
      " [0.12507022]]\n",
      "[[0.3246482 ]\n",
      " [0.23021212]\n",
      " [0.08761953]\n",
      " [0.20711656]\n",
      " [0.35750166]\n",
      " [0.12752591]]\n",
      "[[0.32275118]\n",
      " [0.22813773]\n",
      " [0.0852773 ]\n",
      " [0.20499879]\n",
      " [0.35566635]\n",
      " [0.12525864]]\n",
      "[[0.32450026]\n",
      " [0.23005241]\n",
      " [0.08744202]\n",
      " [0.20695397]\n",
      " [0.35735782]\n",
      " [0.12735338]]\n",
      "[[0.32288223]\n",
      " [0.22828294]\n",
      " [0.0854439 ]\n",
      " [0.20514747]\n",
      " [0.35579247]\n",
      " [0.12541925]]\n",
      "[[0.32437446]\n",
      " [0.22991633]\n",
      " [0.08729042]\n",
      " [0.20681537]\n",
      " [0.3572356 ]\n",
      " [0.12720613]]\n",
      "[[0.32299435]\n",
      " [0.22840695]\n",
      " [0.08558586]\n",
      " [0.20527438]\n",
      " [0.35590046]\n",
      " [0.12555619]]\n",
      "[[0.32426744]\n",
      " [0.22980037]\n",
      " [0.08716097]\n",
      " [0.20669723]\n",
      " [0.35713168]\n",
      " [0.12708045]]\n",
      "[[0.32309025]\n",
      " [0.22851284]\n",
      " [0.08570683]\n",
      " [0.20538271]\n",
      " [0.35599287]\n",
      " [0.12567294]]\n",
      "[[0.32417635]\n",
      " [0.22970153]\n",
      " [0.08705043]\n",
      " [0.2065965 ]\n",
      " [0.35704329]\n",
      " [0.12697319]]\n",
      "[[0.32317222]\n",
      " [0.22860323]\n",
      " [0.08580994]\n",
      " [0.20547517]\n",
      " [0.35607192]\n",
      " [0.12577249]]\n",
      "[[0.3240988 ]\n",
      " [0.22961729]\n",
      " [0.08695608]\n",
      " [0.20651061]\n",
      " [0.35696807]\n",
      " [0.12688166]]\n",
      "[[0.32324228]\n",
      " [0.2286804 ]\n",
      " [0.08589784]\n",
      " [0.20555407]\n",
      " [0.35613951]\n",
      " [0.12585738]]\n",
      "[[0.32403276]\n",
      " [0.22954547]\n",
      " [0.08687553]\n",
      " [0.20643738]\n",
      " [0.35690404]\n",
      " [0.12680356]]\n",
      "[[0.32330214]\n",
      " [0.22874626]\n",
      " [0.08597277]\n",
      " [0.2056214 ]\n",
      " [0.35619728]\n",
      " [0.12592978]]\n",
      "[[0.3239765 ]\n",
      " [0.22948423]\n",
      " [0.08680678]\n",
      " [0.20637493]\n",
      " [0.35684951]\n",
      " [0.12673691]]\n",
      "[[0.32335327]\n",
      " [0.22880248]\n",
      " [0.08603666]\n",
      " [0.20567886]\n",
      " [0.35624664]\n",
      " [0.12599152]]\n",
      "[[0.32392857]\n",
      " [0.22943202]\n",
      " [0.08674811]\n",
      " [0.20632167]\n",
      " [0.35680307]\n",
      " [0.12668005]]\n",
      "[[0.32339694]\n",
      " [0.22885045]\n",
      " [0.08609114]\n",
      " [0.20572789]\n",
      " [0.35628881]\n",
      " [0.12604418]]\n",
      "[[0.32388772]\n",
      " [0.2293875 ]\n",
      " [0.08669804]\n",
      " [0.20627625]\n",
      " [0.3567635 ]\n",
      " [0.12663153]]\n",
      "[[0.32343423]\n",
      " [0.2288914 ]\n",
      " [0.0861376 ]\n",
      " [0.20576973]\n",
      " [0.35632483]\n",
      " [0.12608909]]\n",
      "[[0.32385291]\n",
      " [0.22934953]\n",
      " [0.08665531]\n",
      " [0.20623751]\n",
      " [0.35672978]\n",
      " [0.12659013]]\n",
      "[[0.32346606]\n",
      " [0.22892633]\n",
      " [0.08617721]\n",
      " [0.20580542]\n",
      " [0.35635559]\n",
      " [0.1261274 ]]\n",
      "[[0.32382323]\n",
      " [0.22931715]\n",
      " [0.08661885]\n",
      " [0.20620447]\n",
      " [0.35670105]\n",
      " [0.12655481]]\n",
      "[[0.32349324]\n",
      " [0.22895614]\n",
      " [0.086211  ]\n",
      " [0.20583587]\n",
      " [0.35638185]\n",
      " [0.12616007]]\n",
      "[[0.32379793]\n",
      " [0.22928954]\n",
      " [0.08658774]\n",
      " [0.20617629]\n",
      " [0.35667655]\n",
      " [0.12652468]]\n",
      "[[0.32351644]\n",
      " [0.22898158]\n",
      " [0.08623981]\n",
      " [0.20586186]\n",
      " [0.35640427]\n",
      " [0.12618794]]\n",
      "[[0.32377637]\n",
      " [0.22926599]\n",
      " [0.08656119]\n",
      " [0.20615225]\n",
      " [0.35665568]\n",
      " [0.12649897]]\n",
      "[[0.32353624]\n",
      " [0.22900328]\n",
      " [0.08626439]\n",
      " [0.20588402]\n",
      " [0.3564234 ]\n",
      " [0.12621171]]\n",
      "[[0.32375797]\n",
      " [0.2292459 ]\n",
      " [0.08653854]\n",
      " [0.20613175]\n",
      " [0.35663787]\n",
      " [0.12647704]]\n",
      "[[0.32355313]\n",
      " [0.22902179]\n",
      " [0.08628535]\n",
      " [0.20590294]\n",
      " [0.35643973]\n",
      " [0.12623199]]\n",
      "[[0.32374229]\n",
      " [0.22922876]\n",
      " [0.08651922]\n",
      " [0.20611426]\n",
      " [0.3566227 ]\n",
      " [0.12645833]]\n",
      "[[0.32356755]\n",
      " [0.22903759]\n",
      " [0.08630323]\n",
      " [0.20591907]\n",
      " [0.35645367]\n",
      " [0.12624929]]\n",
      "[[0.32372892]\n",
      " [0.22921415]\n",
      " [0.08650273]\n",
      " [0.20609935]\n",
      " [0.35660975]\n",
      " [0.12644237]]\n",
      "[[0.32357986]\n",
      " [0.22905107]\n",
      " [0.08631848]\n",
      " [0.20593284]\n",
      " [0.35646557]\n",
      " [0.12626404]]\n",
      "[[0.32371751]\n",
      " [0.22920169]\n",
      " [0.08648867]\n",
      " [0.20608662]\n",
      " [0.35659872]\n",
      " [0.12642875]]\n",
      "[[0.32359036]\n",
      " [0.22906257]\n",
      " [0.08633149]\n",
      " [0.20594458]\n",
      " [0.35647572]\n",
      " [0.12627663]]\n",
      "[[0.32370779]\n",
      " [0.22919105]\n",
      " [0.08647667]\n",
      " [0.20607577]\n",
      " [0.35658931]\n",
      " [0.12641713]]\n",
      "[[0.32359931]\n",
      " [0.22907238]\n",
      " [0.08634258]\n",
      " [0.2059546 ]\n",
      " [0.35648438]\n",
      " [0.12628736]]\n",
      "[[0.32369949]\n",
      " [0.22918198]\n",
      " [0.08646643]\n",
      " [0.20606651]\n",
      " [0.35658128]\n",
      " [0.12640722]]\n",
      "[[0.32360696]\n",
      " [0.22908075]\n",
      " [0.08635205]\n",
      " [0.20596314]\n",
      " [0.35649178]\n",
      " [0.12629652]]\n",
      "[[0.32369242]\n",
      " [0.22917425]\n",
      " [0.08645769]\n",
      " [0.20605861]\n",
      " [0.35657443]\n",
      " [0.12639877]]\n",
      "[[0.32361348]\n",
      " [0.22908789]\n",
      " [0.08636012]\n",
      " [0.20597043]\n",
      " [0.35649808]\n",
      " [0.12630433]]\n",
      "[[0.32368638]\n",
      " [0.22916765]\n",
      " [0.08645024]\n",
      " [0.20605187]\n",
      " [0.3565686 ]\n",
      " [0.12639155]]\n",
      "[[0.32361904]\n",
      " [0.22909398]\n",
      " [0.08636701]\n",
      " [0.20597665]\n",
      " [0.35650346]\n",
      " [0.12631099]]\n",
      "[[0.32368123]\n",
      " [0.22916202]\n",
      " [0.08644389]\n",
      " [0.20604613]\n",
      " [0.35656362]\n",
      " [0.1263854 ]]\n",
      "[[0.32362379]\n",
      " [0.22909917]\n",
      " [0.08637288]\n",
      " [0.20598196]\n",
      " [0.35650805]\n",
      " [0.12631668]]\n",
      "[[0.32367684]\n",
      " [0.22915722]\n",
      " [0.08643846]\n",
      " [0.20604122]\n",
      " [0.35655937]\n",
      " [0.12638015]]\n",
      "[[0.32362784]\n",
      " [0.22910361]\n",
      " [0.08637789]\n",
      " [0.20598649]\n",
      " [0.35651197]\n",
      " [0.12632153]]\n"
     ]
    }
   ],
   "source": [
    "x_t = x_0.reshape(-1,1)\n",
    "dt = 1\n",
    "print(x_0)\n",
    "for i in range(100):\n",
    "    x_t = x_t + (-x_t + np.matmul(\n",
    "                                np.matmul(np.transpose(x_t), np.matmul(A, x_t))\n",
    "                                + ( 1 - np.matmul(np.transpose(x_t),np.matmul(A, x_t)))\n",
    "                                * np.identity(np.size(x_t, 0))\n",
    "                                , x_t))*dt\n",
    "    print(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.50616948e-05]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.T @ A @ x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00403822]\n",
      " [-0.72693076]\n",
      " [ 0.45086439]\n",
      " [ 0.45165648]\n",
      " [-0.24859087]\n",
      " [ 0.04985641]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.28431894]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from eigh\n",
    "x_t = eigvec[:,-1].reshape(-1,1)\n",
    "print(x_t)\n",
    "x_t.T @ A @ x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
