{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as skl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"../src_fork/\")\n",
    "sys.path.insert(0,\"../src_fork/analysis\")\n",
    "\n",
    "import linear_regression\n",
    "import utils\n",
    "import stat_tools\n",
    "import crossvalidation\n",
    "import bootstrap\n",
    "from FrankeFunction import FrankeFunction\n",
    "\n",
    "from imageio import imread\n",
    "\n",
    "\n",
    "utils.plot_settings()  # LaTeX fonts in Plots!\n",
    "\n",
    "FIGPATH = \"../figs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the terrain data:\n",
    "# Note structure! X-coordinates are on the rows of terrain_data\n",
    "# Point_selection.flatten() moves most rapidly over the x-coordinates\n",
    "# Meshgrids flattened also move most rapidly over the x-coordinates. Thus\n",
    "# this should make z(x,y).reshape(length_y,length_x) be consistent with terrain_data\n",
    "\n",
    "\n",
    "terrain_data = imread(\"../datafiles/SRTM_data_Norway_1.tif\")\n",
    "point_selection = terrain_data[800:1800:20, 800:1800:20]  # Make quadratic and downsample\n",
    "x_terrain_selection = np.linspace(0, 1, point_selection.shape[1])\n",
    "y_terrain_selection = np.linspace(0, 1, point_selection.shape[0])\n",
    "X_coord_selection, Y_coord_selection = np.meshgrid(x_terrain_selection, y_terrain_selection)\n",
    "z_terrain_selection = point_selection.flatten()  # the response values\n",
    "x_terrain_selection_flat = X_coord_selection.flatten()  # the first degree feature variables\n",
    "y_terrain_selection_flat = Y_coord_selection.flatten()  # the first degree feature variables\n",
    "\n",
    "\n",
    "max_degree = 25\n",
    "n_lambdas = 15\n",
    "n_bootstraps = 50\n",
    "k_folds = 5\n",
    "lambdas = np.logspace(-5, 1, n_lambdas)\n",
    "subset_lambdas = lambdas[::5]\n",
    "\n",
    "\n",
    "x = x_terrain_selection_flat\n",
    "y = y_terrain_selection_flat\n",
    "z = z_terrain_selection\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(x, y, z, test_size=0.2)\n",
    "\n",
    "# Centering\n",
    "z_intercept = np.mean(z)\n",
    "z = z - z_intercept\n",
    "\n",
    "z_train_intercept = np.mean(z_train)\n",
    "z_train = z_train - z_train_intercept\n",
    "z_test = z_test - z_train_intercept\n",
    "\n",
    "# Show the terrain\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.title(f\"Full image ({terrain_data.size} px)\")\n",
    "plt.imshow(terrain_data, cmap=\"gray\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(point_selection, cmap=\"gray\")\n",
    "plt.title(f\"Subsection ({point_selection.size} px)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def p1_analysis(x, y, z, degrees):\n",
    "    mse = pd.DataFrame(columns=[\"train\", \"test\"], index = degrees)\n",
    "    r2 = pd.DataFrame(columns=[\"train\", \"test\"], index = degrees)\n",
    "    \n",
    "    max_num_betas = int((degrees[-1] + 1) * (degrees[-1] + 2) / 2)\n",
    "    betas = np.zeros([len(degrees), max_num_betas])\n",
    "    var_betas = np.zeros([len(degrees), max_num_betas])\n",
    "\n",
    "    for i, deg in enumerate(degrees):\n",
    "        X = linear_regression.design_matrix_2D(x, y, deg)\n",
    "        # Split data, but don't shuffle. OK since data is already randomly sampled! \n",
    "        # Fasilitates a direct comparrison of the clean & Noisy data\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size = 0.2, shuffle=True)\n",
    "        # Normalize data sets\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_train[:, 0] = np.ones(X_train.shape[0])\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_test[:, 0] = np.ones(X_test.shape[0])\n",
    "\n",
    "        beta = linear_regression.OLS_SVD_2D(X_train, z_train)\n",
    "    \n",
    "        mse[\"train\"][i] = stat_tools.MSE(z_train, X_train @ beta)\n",
    "        mse[\"test\"][i] = stat_tools.MSE(z_test, X_test @ beta)\n",
    "        r2[\"train\"][i] = stat_tools.R2(z_train, X_train @ beta)\n",
    "        r2[\"test\"][i] = stat_tools.R2(z_test, X_test @ beta)\n",
    "        \n",
    "        betas[i, 0:len(beta)] = beta\n",
    "        var_betas[i, 0:len(beta)] = stat_tools.var_beta(X_train)\n",
    "\n",
    "    return mse, r2, betas, var_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(0, 20) # Degrees of the polynom\n",
    "\n",
    "# Generate data\n",
    "MSE_clean, R2_clean, betas, var_betas = p1_analysis(x, y, z, degrees)\n",
    "\n",
    "plt.figure(figsize=(5,8))\n",
    "plt.tight_layout()\n",
    "# Plot MSE\n",
    "plt.subplot(211)\n",
    "plt.semilogy(degrees, MSE_clean[\"train\"],\"o--\", label = \"Training data\")\n",
    "plt.semilogy(degrees, MSE_clean[\"test\"],\"o--\", label = \"Test data\")\n",
    "plt.xlabel(\"Model Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "# Plot R2\n",
    "plt.subplot(212)\n",
    "plt.plot(degrees, R2_clean[\"train\"],\"o--\", label = \"Training data\")\n",
    "plt.plot(degrees, R2_clean[\"test\"],\"o--\", label = \"Test data\")\n",
    "plt.xlabel(\"Model Complexity\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setup of problem is completede above.\n",
    "\n",
    "\n",
    "# Quantities of interest:\n",
    "mse_ols_test = np.zeros(max_degree)\n",
    "mse_ols_train = np.zeros(max_degree)\n",
    "ols_cv_mse = np.zeros(max_degree)\n",
    "\n",
    "ols_boot_mse = np.zeros(max_degree)\n",
    "ols_boot_bias = np.zeros(max_degree)\n",
    "ols_boot_variance = np.zeros(max_degree)\n",
    "\n",
    "best_ridge_lambda = np.zeros(max_degree)\n",
    "best_ridge_mse = np.zeros(max_degree)\n",
    "ridge_best_lambda_boot_mse = np.zeros(max_degree)\n",
    "ridge_best_lambda_boot_bias = np.zeros(max_degree)\n",
    "ridge_best_lambda_boot_variance = np.zeros(max_degree)\n",
    "\n",
    "best_lasso_lambda = np.zeros(max_degree)\n",
    "best_lasso_mse = np.zeros(max_degree)\n",
    "lasso_best_lambda_boot_mse = np.zeros(max_degree)\n",
    "lasso_best_lambda_boot_bias = np.zeros(max_degree)\n",
    "lasso_best_lambda_boot_variance = np.zeros(max_degree)\n",
    "\n",
    "ridge_lamb_deg_mse = np.zeros((max_degree, n_lambdas))\n",
    "lasso_lamb_deg_mse = np.zeros((max_degree, n_lambdas))\n",
    "\n",
    "ridge_subset_lambda_boot_mse = np.zeros((max_degree, len(subset_lambdas)))\n",
    "ridge_subset_lambda_boot_bias = np.zeros((max_degree, len(subset_lambdas)))\n",
    "ridge_subset_lambda_boot_variance = np.zeros((max_degree, len(subset_lambdas)))\n",
    "lasso_subset_lambda_boot_mse = np.zeros((max_degree, len(subset_lambdas)))\n",
    "lasso_subset_lambda_boot_bias = np.zeros((max_degree, len(subset_lambdas)))\n",
    "lasso_subset_lambda_boot_variance = np.zeros((max_degree, len(subset_lambdas)))\n",
    "\n",
    "\n",
    "# Actual computations\n",
    "for degree in range(max_degree):\n",
    "    X = linear_regression.design_matrix_2D(x, y, degree)\n",
    "    X_train = linear_regression.design_matrix_2D(x_train, y_train, degree)\n",
    "    X_test = linear_regression.design_matrix_2D(x_test, y_test, degree)\n",
    "    # Scaling and feeding to CV.\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    #    X_scaled[:,0] = 1 # Probably should not have this.\n",
    "\n",
    "    # Scaling and feeding to bootstrap and OLS\n",
    "    scaler_boot = StandardScaler()\n",
    "    scaler_boot.fit(X_train)\n",
    "    X_train_scaled = scaler_boot.transform(X_train)\n",
    "    X_test_scaled = scaler_boot.transform(X_test)\n",
    "    #    X_train_scaled[:,0] = 1 # Probably actually not\n",
    "    #    X_test_scaled[:,0] = 1 # Have a bad feeling about how this might affect ridge/lasso.\n",
    "\n",
    "    # OLS, get MSE for test and train set.\n",
    "\n",
    "    betas = linear_regression.OLS_SVD_2D(X_train_scaled, z_train)\n",
    "    z_test_model = X_test_scaled @ betas\n",
    "    z_train_model = X_train_scaled @ betas\n",
    "    mse_ols_train[degree] = stat_tools.MSE(z_train, z_train_model)\n",
    "    mse_ols_test[degree] = stat_tools.MSE(z_test, z_test_model)\n",
    "\n",
    "    # CV, find best lambdas and get mse vs lambda for given degree.\n",
    "\n",
    "    lasso_cv_mse, ridge_cv_mse, ols_cv_mse_deg = crossvalidation.k_fold_cv_all(\n",
    "        X_scaled, z, n_lambdas, lambdas, k_folds\n",
    "    )\n",
    "    best_lasso_lambda[degree] = lambdas[np.argmin(lasso_cv_mse)]\n",
    "    best_ridge_lambda[degree] = lambdas[np.argmin(ridge_cv_mse)]\n",
    "    best_lasso_mse[degree] = np.min(lasso_cv_mse)\n",
    "    best_ridge_mse[degree] = np.min(ridge_cv_mse)\n",
    "    lasso_lamb_deg_mse[degree] = lasso_cv_mse\n",
    "    ridge_lamb_deg_mse[degree] = ridge_cv_mse\n",
    "    ols_cv_mse[degree] = ols_cv_mse_deg\n",
    "    # All regression bootstraps at once\n",
    "    #\n",
    "    #\n",
    "    lamb_ridge = best_ridge_lambda[degree]\n",
    "    lamb_lasso = best_lasso_lambda[degree]\n",
    "    #\n",
    "    ridge_mse, ridge_bias, ridge_variance, lasso_mse, lasso_bias, lasso_variance, ols_mse, ols_bias, ols_variance = \\\n",
    "    bootstrap.bootstrap_all(X_train_scaled, X_test_scaled, z_train, z_test, n_bootstraps, lamb_lasso, lamb_ridge)\n",
    "    #\n",
    "    ridge_best_lambda_boot_mse[degree], ridge_best_lambda_boot_bias[degree], \\\n",
    "    ridge_best_lambda_boot_variance[degree] = ridge_mse, ridge_bias, ridge_variance\n",
    "    #\n",
    "    lasso_best_lambda_boot_mse[degree], lasso_best_lambda_boot_bias[degree], \\\n",
    "    lasso_best_lambda_boot_variance[degree] = lasso_mse, lasso_bias, lasso_variance\n",
    "    #\n",
    "    ols_boot_mse[degree], ols_boot_bias[degree], \\\n",
    "    ols_boot_variance[degree] = ols_mse, ols_bias, ols_variance\n",
    "\n",
    "    # Bootstrapping for a selection of lambdas for ridge and lasso\n",
    "    subset_lambda_index = 0\n",
    "    for lamb in subset_lambdas:\n",
    "    #\n",
    "        ridge_mse, ridge_bias, ridge_variance, lasso_mse, lasso_bias, lasso_variance = \\\n",
    "        bootstrap.bootstrap_ridge_lasso(X_train_scaled, X_test_scaled, z_train, z_test, n_bootstraps, lamb_lasso, lamb_ridge)\n",
    "    #\n",
    "        ridge_subset_lambda_boot_mse[degree, subset_lambda_index ], ridge_subset_lambda_boot_bias[degree, subset_lambda_index ], \\\n",
    "        ridge_subset_lambda_boot_variance[degree, subset_lambda_index ] = ridge_mse, ridge_bias, ridge_variance\n",
    "    #\n",
    "        lasso_subset_lambda_boot_mse[degree, subset_lambda_index ], lasso_subset_lambda_boot_bias[degree, subset_lambda_index ], \\\n",
    "        lasso_subset_lambda_boot_variance[degree, subset_lambda_index ] = lasso_mse, lasso_bias, lasso_variance\n",
    "    #\n",
    "        subset_lambda_index  += 1\n",
    "    #\n",
    "\n",
    "\n",
    "# Plots go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(max_degree)\n",
    "plt.semilogy(degrees, mse_ols_test, \"o--\", label = \"OLS TEST\")\n",
    "plt.semilogy(degrees, mse_ols_train, \"o--\", label = \"OLS TRAIN\")\n",
    "plt.title(\"OLS\")\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BOOTSTRAP PLOTS\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.semilogy(degrees, ols_boot_mse,\"o--\",label=\"MSE\")\n",
    "plt.semilogy(degrees, ols_boot_variance,\"--\", label=\"Var\")\n",
    "plt.semilogy(degrees, ols_boot_bias,\"--\", label=\"Bias$^2$\")\n",
    "plt.title(\"OLS Bootstrap\")\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.semilogy(degrees, ridge_best_lambda_boot_mse,\"o--\",label=\"MSE\")\n",
    "plt.semilogy(degrees, ridge_best_lambda_boot_variance,\"--\", label=\"Var\")\n",
    "plt.semilogy(degrees, ridge_best_lambda_boot_bias,\"--\", label=\"Bias$^2$\")\n",
    "plt.title(\"Ridge Bootstrap\")\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.semilogy(degrees, lasso_best_lambda_boot_mse,\"o--\",label=\"MSE\")\n",
    "plt.semilogy(degrees, lasso_best_lambda_boot_bias,\"o--\", label=\"BIAS\")\n",
    "plt.semilogy(degrees, lasso_best_lambda_boot_variance,\"o--\", label=\"Variance\")\n",
    "plt.title(\"LASSO Bootstrap\")\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.semilogy(degrees, ols_boot_mse,\"o--\",label=\"OLS MSE\")\n",
    "plt.semilogy(degrees, ridge_best_lambda_boot_mse,\"o--\",label=\"RIDGE MSE\")\n",
    "plt.semilogy(degrees, lasso_best_lambda_boot_mse,\"o--\",label=\"LASSO MSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same bootstrap plots, but only better resolved LASSO & Ridge\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(degrees, ridge_best_lambda_boot_mse,\"o--\",label=\"MSE\")\n",
    "plt.semilogy(degrees, ridge_best_lambda_boot_bias,\"o--\", label=\"BIAS\")\n",
    "plt.semilogy(degrees, ridge_best_lambda_boot_variance,\"o--\", label=\"Variance\")\n",
    "plt.title(\"Ridge Bootstrap\")\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "#plt.ylim([1e-4, 1e-1])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.semilogy(degrees, lasso_best_lambda_boot_mse,\"o--\",label=\"MSE\")\n",
    "plt.semilogy(degrees, lasso_best_lambda_boot_bias,\"o--\", label=\"BIAS\")\n",
    "plt.semilogy(degrees, lasso_best_lambda_boot_variance,\"o--\", label=\"Variance\")\n",
    "plt.title(\"LASSO Bootstrap\")\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "#plt.ylim([1e-4, 1e-1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTOUR PLOTS\n",
    "\n",
    "# RIDGE\n",
    "plt.figure(figsize=(4, 3))\n",
    "X, Y = np.meshgrid(np.log10(lambdas), np.arange(max_degree))\n",
    "plt.contourf(X, Y, np.log10(ridge_lamb_deg_mse), cmap=cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"$\\\\log_{10}(\\lambda)$\")\n",
    "plt.ylabel(\"Complexity\")\n",
    "plt.title(\"$\\log_{10}$MSE\")\n",
    "plt.savefig(FIGPATH + \"RIDGE_CV_Franke_contour.pdf\")\n",
    "\n",
    "# LASSO\n",
    "plt.figure(figsize=(4, 3))\n",
    "X, Y = np.meshgrid(np.log10(lambdas), np.arange(max_degree))\n",
    "plt.contourf(X, Y, np.log10(lasso_lamb_deg_mse), cmap=cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"$\\\\log_{10}(\\lambda)$\")\n",
    "plt.ylabel(\"Complexity\")\n",
    "plt.title(\"$\\log_{10}$MSE\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGPATH + \"LASSO_CV_Franke_contour.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
