{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"../src/\")\n",
    "sys.path.insert(0,\"../tests/\")\n",
    "\n",
    "import linear_regression\n",
    "import utils\n",
    "import unit_tests\n",
    "import linear_regression\n",
    "import stat_tools\n",
    "from FrankeFunction import FrankeFunction\n",
    "\n",
    "\n",
    "FIGPATH = \"../figs/\"\n",
    "FIGURESIZE = np.array([4,3])\n",
    "\n",
    "# Make sure things are working as expected\n",
    "unit_tests.OLS_unit_test()\n",
    "unit_tests.OLS_SVD_unit_test()\n",
    "unit_tests.Ridge_unit_test()\n",
    "\n",
    "SEEDVAL = 2021\n",
    "np.random.seed(SEEDVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "n = 500 # Number of data points\n",
    "\n",
    "x_dat = np.random.uniform(0, 1, n)\n",
    "y_dat = np.random.uniform(0, 1, n)\n",
    "z_dat = FrankeFunction(x_dat, y_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the analysis in a function to duplicate for Clean & Noisy data\n",
    "def p1_analysis(x, y, z, degrees):\n",
    "    mse = pd.DataFrame(columns=[\"train\", \"test\"], index = degrees)\n",
    "    r2 = pd.DataFrame(columns=[\"train\", \"test\"], index = degrees)\n",
    "    \n",
    "    max_num_betas = int((degrees[-1] + 1) * (degrees[-1] + 2) / 2)\n",
    "    betas = np.zeros([len(degrees), max_num_betas])\n",
    "    var_betas = np.zeros([len(degrees), max_num_betas])\n",
    "\n",
    "    for i, deg in enumerate(degrees):\n",
    "        X = linear_regression.design_matrix_2D(x, y, deg)\n",
    "        # Split data, but don't shuffle. OK since data is already randomly sampled! \n",
    "        # Fasilitates a direct comparrison of the clean & Noisy data\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X, z, test_size = 0.2, shuffle=False)\n",
    "        # Normalize data sets\n",
    "        scaler = StandardScaler()        \n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_train[:, 0] = np.ones(X_train.shape[0])\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_test[:, 0] = np.ones(X_test.shape[0])\n",
    "\n",
    "        beta = linear_regression.OLS_SVD_2D(X_train, z_train)\n",
    "    \n",
    "        mse[\"train\"][i] = stat_tools.MSE(z_train, X_train @ beta)\n",
    "        mse[\"test\"][i] = stat_tools.MSE(z_test, X_test @ beta)\n",
    "        r2[\"train\"][i] = stat_tools.R2(z_train, X_train @ beta)\n",
    "        r2[\"test\"][i] = stat_tools.R2(z_test, X_test @ beta)\n",
    "        \n",
    "        betas[i, 0:len(beta)] = beta\n",
    "        var_betas[i, 0:len(beta)] = stat_tools.var_beta(X_train, 0.2)\n",
    "\n",
    "    return mse, r2, betas, var_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 5\n",
    "\n",
    "degrees = np.arange(0, c+1) # Degrees of the polynom\n",
    "\n",
    "# Generate data\n",
    "MSE_clean, R2_clean, betas, var_betas = p1_analysis(\n",
    "    x_dat, y_dat, z_dat + np.random.normal(0, 1, n) * 0.2, degrees\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5,8))\n",
    "plt.tight_layout()\n",
    "# Plot MSE\n",
    "plt.subplot(211)\n",
    "plt.semilogy(degrees[1:], MSE_clean[\"train\"][1:],\"o--\", label = \"Training data\")\n",
    "plt.semilogy(degrees[1:], MSE_clean[\"test\"][1:],\"o--\", label = \"Test data\")\n",
    "plt.xlabel(\"Model Complexity\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "# Plot R2\n",
    "plt.subplot(212)\n",
    "plt.plot(degrees[1:], R2_clean[\"train\"][1:],\"o--\", label = \"Training data\")\n",
    "plt.plot(degrees[1:], R2_clean[\"test\"][1:],\"o--\", label = \"Test data\")\n",
    "plt.xlabel(\"Model Complexity\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "\n",
    "for c in range(0, c+1):\n",
    "    indx = np.arange(0, utils.polynomial_no_terms(c))\n",
    "    err =  4 * np.sqrt(var_betas[c][:utils.polynomial_no_terms(c)])\n",
    "    plt.plot(indx[1:], err[1:], \"--\", label=f\"p = {c}\")\n",
    "\n",
    "plt.ylabel(\"$4\\\\sigma$ (95\\%)\")\n",
    "plt.xticks(np.arange(1, 21), utils.polynomial_form(5)[1:], rotation=20)\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGPATH+\"beta_confidence_interval_size_OLS.pdf\")\n",
    "\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.errorbar(indx[1:], betas[c][1:utils.polynomial_no_terms(c)],\n",
    "            yerr=err[1:],\n",
    "            fmt=\"o\",\n",
    "            capsize=5\n",
    "            )\n",
    "#plt.ylim([-10, 10])\n",
    "#plt.xlabel(\"Term\")\n",
    "\"\"\"\"\n",
    "terms = utils.polynomial_form(c)\n",
    "for i in range(21):\n",
    "    plt.annotate(terms[i], xy=(i, betas[c][i]))\n",
    "\"\"\"\n",
    "\n",
    "plt.ylabel(\"$\\\\beta_i$\")\n",
    "plt.grid(\"on\")\n",
    "plt.xticks(np.arange(0, 21), utils.polynomial_form(c), rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGPATH+\"beta_confidence_interval_OLS.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean confidence interval for first 5 polynomial degrees\n",
    "err = []\n",
    "for c in range(1, c+1):\n",
    "    err.append(np.mean(4 * np.sqrt(var_betas[c][:utils.polynomial_no_terms(c)])))\n",
    "\n",
    "coeffs = np.polyfit(range(1, c+1), err, 2)\n",
    "plt.figure(figsize=FIGURESIZE)\n",
    "xvals = np.linspace(1, 5, 100)\n",
    "plt.plot(xvals, coeffs[0] *xvals**2 + coeffs[1] * xvals + coeffs[2], \"--\", color=\"C0\")\n",
    "plt.plot(range(1, c+1), err, \"o\")\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"Mean confidence interval of $\\\\beta_i$ (95\\%)\")\n",
    "plt.xticks(np.arange(1, 5+1))\n",
    "plt.savefig(FIGPATH + \"Franke_mean_confidence_interval.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = np.linspace(0, .5, 6)\n",
    "degrees = np.arange(0, 6)\n",
    "\n",
    "plt.figure(figsize=[4, 3.5])\n",
    "\n",
    "for noise in noises:\n",
    "    # Generate a new noisy dataset\n",
    "    z_dat_noisy = z_dat + np.random.normal(0, 1, n) * noise\n",
    "    MSE_noisy, R2_noisy, _, _ = p1_analysis(x_dat, y_dat, z_dat_noisy,degrees)\n",
    "    \n",
    "    plt.plot(degrees, R2_noisy[\"test\"], \"o--\")\n",
    "    plt.annotate(\"$\\delta=$ %.2f\" % noise, [degrees[-1] + .3,R2_noisy[\"test\"].iloc[-1]])\n",
    "\n",
    "plt.xlim([0, degrees[-1] + 1.5])\n",
    "plt.axhline([1],color=\"black\", ls=\"--\", lw=0.75)\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.savefig(FIGPATH+\"OLS_R2_noise.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
