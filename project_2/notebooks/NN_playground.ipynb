{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "sys.path.insert(0, \"../../project_1/src\")\n",
    "from NeuralNetwork import *\n",
    "from CostFunctions import *\n",
    "from ActivationFunctions import *\n",
    "from SGD import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from FrankeFunction import *\n",
    "from linear_regression import *\n",
    "from stat_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "https://scikit-learn.org/stable/modules/neural_networks_supervised.html#tips-on-practical-use\n",
    "\n",
    "- Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the same scaling to the test set for meaningful results. You can use StandardScaler for standardization.\n",
    "\n",
    "Hastie p400\n",
    "- Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used. Typically the number of hidden units is somewhere in the range of 5 to 100, with the number increasing with the number of inputs and number of training cases. It is most common to put down a reasonably large number of units and train them with regularization. Some researchers use cross-validation to estimate the optimal number, but this seems unnecessary if cross-validation is used to estimate the regularization parameter.\n",
    "\n",
    "- Since the scaling of the inputs determines the effective scaling of the weights in the bottom layer, it can have a large effect on the quality of the final solution. At the outset it is best to standardize all inputs to have mean zero and standard deviation one. This ensures all inputs are treated equally in the regularization process, and allows one to choose a meaningful range for the random starting weights. With standardized inputs, it is typical to take random uniform weights over the range [âˆ’0.7, +0.7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(2020)\n",
    "N = 1000\n",
    "\n",
    "#xy = np.random.uniform(0, 1, (N, 2))\n",
    "resolution = 25\n",
    "x = np.linspace(0, 1, resolution)\n",
    "y = np.linspace(0, 1, resolution)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "X_flat, Y_flat = X.flatten(), Y.flatten()\n",
    "xy = np.array([X_flat, Y_flat]).T\n",
    "\n",
    "N = len(xy)\n",
    "print(N)\n",
    "\n",
    "z = FrankeFunction(xy[:, 0], xy[:, 1]) +  np.random.normal(0, 1, N) * 0.2\n",
    "z = z.reshape(-1,1)\n",
    "\n",
    "xy_train, xy_test, z_train, z_test = train_test_split(xy, z, test_size=0.2)\n",
    "\n",
    "# Rescale & center data wrt. training data\n",
    "#z_train_intercept = np.mean(z_train)\n",
    "#z_train -= z_train_intercept\n",
    "#z_test -= z_train_intercept\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(xy_train)\n",
    "#xy_train = scaler.transform(xy_train)\n",
    "#xy_test = scaler.transform(xy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_loop = 100\n",
    "tot_epochs = 10_000\n",
    "num_loops = int(tot_epochs / per_loop)\n",
    "\n",
    "NN_MSE = np.empty(num_loops)\n",
    "NN_epochs = np.empty(num_loops)\n",
    "\n",
    "FFNN = FeedForwardNeuralNetwork(\n",
    "    X=xy_train,\n",
    "    Y=z_train,\n",
    "    cost=CostFunctions.SquareError,\n",
    "    activation=ActivationFunctions.ELU,\n",
    "    activation_out=ActivationFunctions.ID,\n",
    "    network_shape=[30]\n",
    ")\n",
    "\n",
    "for i in range(num_loops):\n",
    "    FFNN.train(N_minibatches=int(N/32), learning_rate=0.01, n_epochs=per_loop)\n",
    "    NN_MSE[i] = MSE(z_test, FFNN.predict(xy_test))\n",
    "    NN_epochs[i] = FFNN.total_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_model = MLPRegressor(\n",
    "    hidden_layer_sizes = [30],\n",
    "    activation = 'relu',\n",
    "    solver = 'sgd',\n",
    "    batch_size = 32,\n",
    "    learning_rate_init = 0.1,\n",
    "    max_iter = 10_000,\n",
    "    tol=1e-6,\n",
    "    momentum = 0,\n",
    "    early_stopping = False,\n",
    "    alpha = 0\n",
    ")\n",
    "skl_model.fit(xy_train, z_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outgoing activation: \",skl_model.out_activation_)\n",
    "print(\"No. Itterations: \", skl_model.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(131)\n",
    "plt.plot(NN_epochs, NN_MSE)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.subplot(132)\n",
    "plt.semilogy(NN_epochs, NN_MSE)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.subplot(133)\n",
    "plt.scatter(xy[:, 0], xy[:, 1])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN.train(N_minibatches=int(500/32), learning_rate=0.01, n_epochs=per_loop)\n",
    "print(FFNN.total_epochs)# Predict using our class\n",
    "resolution = 100\n",
    "x = np.linspace(0, 1, resolution)\n",
    "y = np.linspace(0, 1, resolution)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "X_flat, Y_flat = X.flatten(), Y.flatten()\n",
    "XY_flat = np.array([X_flat, Y_flat]).T\n",
    "#XY_flat_scaled = scaler.fit(XY_flat)\n",
    "Z_pred = FFNN.predict(XY_flat)\n",
    "Z_pred = Z_pred.T.reshape(X.shape) # + z_train_intercept\n",
    "Z_Franke = FrankeFunction(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using Scikit\n",
    "z_skl_pred = skl_model.predict(XY_flat)\n",
    "z_skl_pred = z_skl_pred.T.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 100\n",
    "x = np.linspace(0, 1, resolution)\n",
    "y = np.linspace(0, 1, resolution)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "X_flat, Y_flat = X.flatten(), Y.flatten()\n",
    "XY_flat = np.array([X_flat, Y_flat]).T\n",
    "#XY_flat_scaled = scaler.fit(XY_flat)\n",
    "Z_pred = FFNN.predict(XY_flat)\n",
    "Z_pred = Z_pred.T.reshape(X.shape) # + z_train_intercept\n",
    "Z_Franke = FrankeFunction(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FFNN.train(N_minibatches=int(N/32), learning_rate=0.1, n_epochs=5_000)\n",
    "print(FFNN.total_epochs)# Predict using our class\n",
    "\n",
    "elev = 20\n",
    "azim = 40\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1, projection=\"3d\")\n",
    "surf = ax.plot_surface(X, Y, Z_pred, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax.view_init(elev=elev, azim=azim)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_zlim([0, 1.2])\n",
    "ax.set_title(\"Neural Net\")\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2, projection=\"3d\")\n",
    "surf = ax.plot_surface(X, Y, Z_Franke, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax.view_init(elev=elev, azim=azim)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_zlim([0, 1.2])\n",
    "ax.set_title(\"Franke Function\")\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3, projection=\"3d\")\n",
    "surf = ax.plot_surface(X, Y, z_skl_pred, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax.view_init(elev=elev, azim=azim)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_zlim([0, 1.2])\n",
    "ax.set_title(\"SKLearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FFNN.biases[0], bins=30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
