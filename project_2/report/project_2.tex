\documentclass[reprint, english, nofootinbib]{revtex4-2}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsbsy}
%\usepackage{bbold}
\usepackage{subfig}

\usepackage{blindtext}
\usepackage{tikzducks}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}

\graphicspath{{../figs/}}

\begin{document}
\title{Classification and Regression\\
\normalsize{From Linear and Logistic Regression to Neural Networks}}

\author{Nicholas Karlsen}
\affiliation{University of Oslo}
\author{Thore Espedal Moe}
\affiliation{University of Oslo}
\date{\today}

\begin{abstract}
\end{abstract}

\maketitle

\section{Introduction}

\section{Theory}
\subsection{Gradient Descent}
\noindent
Consider a cost function in the form
\begin{equation}
    C(\pmb Y, \tilde{\pmb Y}(\pmb w)) = \frac{1}{N}\sum_{i=1}^{N}c_i(\pmb y_i, \tilde{\pmb y}(\pmb w)_i)
\end{equation}
which quantifies the error for some data set $\pmb Y = \left\{\pmb y_1, \dots, \pmb y_N\right\}$ with respect to a corresponding set of modeled data $\tilde{\pmb Y}(\pmb w) = \left\{\tilde{\pmb y_1}(\pmb w), \dots, \tilde{\pmb y}_N(\pmb w)\right\}$, where $\pmb w$ denotes the free parameters of the model.

Since we can not in general expect to have a way of minimizing the cost analytically, we may in stead optimize it by gradient descent (GD), where the set of free parameters $\pmb w$ are initialized in some way, and then incremented by
\begin{equation}\label{eqn: GD}
    \pmb w^{(k+1)} = \pmb w^{(k)} - \eta \nabla_{w}C\qty(\pmb Y, \tilde{\pmb Y}(\pmb w))
\end{equation}
for either a set number of epochs or in practice, until $\norm{\pmb w^{(k+1)} - \pmb w^{(k)}}_2< \varepsilon$, some tolerance. We also have the parameter $\eta \in \mathbb R_{>0}$, often called the learning rate, which can either be constant, or change wrt. $k$.

A notable, and very substantial drawback of GD is that it will always converge to a local minima for a given data set \& initial weights, which for a particularly complicated high dimensional parameter space can not be expected to correspond to the global minima in general. This issue, along with many others \cite[pp.15-16]{Mehta_2019} motivates modifications to the GD method.

\subsubsection{Stochastic Gradient Descent}
\noindent
One attempt at remedying the problems of GD is the so-called \textit{Stochastic Gradient Descent} (SGD), in which stochasticity is introduced to the gradient descent by instead of performing GD on the entirety of $\pmb Y$, it is performed on individual, randomly sampled points $\pmb y_i$ with replacement, and updating the weights for each individual sample.
It turns out that doing SGD in this way generally leads to faster, and better convergence compared to the regular GD, with the added benefit of being computationally faster. It is worth noting however that it is often observed that doing SGD without replacement can yield faster convergence, and is in practice often done this way. The exact reasoning behind this remaining an open question \cite{shamir2016withoutreplacement}\cite{pmlr-v97-nagaraj19a}. For simplicity, we have opted to conform to this norm and will be performing SGD without replacement.

\subsubsection{SGD with Mini-Batches}
\noindent
Yet another modification of the SGD consists of performing the SGD on random subsets $\pmb Y_{MB} \subset \pmb Y$, rather than individual points $\pmb y_i$. These subsets are often called \textit{Mini-Batches} (MB), and their introduction may again improve the performance of SGD. To elaborate, this is performed by randomly splitting $\pmb Y$ into a set of mini-batches where each subset has $\approx N_{B}$ elements\footnote{If $\pmb Y$ is not exactly divisible, one may simply distribute the extra $\pmb y_i$ equally among the batches}. GD is then performed on each of the subsets, updating the weights each time. Once the subset has been exhausted, $\pmb Y$ is again randomly sampled to construct another similar set of mini-batches, where each shuffling of the mini-batches constitutes an epoch of the algorithm.

The choice of $N_{B}$ somewhat decided by trial and error and is also strongly linked to the choice of learning rate $\nu$. However, it is usually chosen as relatively small number relative to the full dataset as to yield the benefits of the SGD method. One may also, as discussed in this excellent lecture by \textcite{ManyBodyML} intuitively link SGD to statistical mechanics, with the ratio $\eta / N_{B} \propto T$; representing an "effective temperature". Thus having small mini-batch sizes corresponds to a high effective temperature; where the system will be able to explore a much larger portion of the parameter space. Then by either increasing $N_B$, or decreasing $\eta$ one may slowly lower the effective temperature; annealing the system into the global minima. For the full justification of this view; we refer the reader to \textcite{ManyBodyML}.

Common choices for $N_B$ is often in the order of $\sim 10-100$, with larger batches often yields diminishing returns in terms of the performance gain along with a larger computational cost. Further, choosing batch sizes as powers of 2 may yield slight performance gain by virtue of how computer hardware works \cite{Aggarwall}.

\subsubsection{Learning Rate Adjustment}
\noindent
As somewhat motivated by the intuition motivated in the previous section, it is often quite useful to be able to adjust the learning rate of SGD over the epochs, as this will in principle allow the algorithm to initially very rapidly explore the parameter space, prior to relaxing slowly into the (hopefully) global minima as the learning rate is decreased.

One such scheme is the inverse decay, where the learning rate evolves like
\begin{equation}
    \eta\qty(t) = \frac{\eta_0}{1 + \gamma t}
\end{equation}
where $\eta_0$ is the initial learning rate, $t$ the current epoch and the decay rate $\gamma$ which controls the rate at which the learning rate will decrease. Similarly; we may also evolve the learning rate by as exponential decay as
\begin{equation}
    \eta(t) = \eta_0 e^{-\gamma t}
\end{equation}
where the appropriate choice must be determined on a per-dataset basis as the convergence of SGD will vary wildly. An additional, much more involved technique is to run the SGD for a certain number of epochs, then manually readjusting the learning rate, either as a constant or by the above methods, then running for another set of epochs. Whilst this sort of technique where one manually hand-tunes the hyper-parameters requires much attention from the user, it may yield good results and even enable some of the simpler schemes to outperform some of the more sophisticated, adaptive schemes \cite{zhang2018yellowfin}.

\subsubsection{SGD Variants}
\noindent
Beyond the basic iteration defined by Eqn.~\ref{eqn: GD}, there exists a large number of alterations beyond just scaling the learning rate which in some way aim to improve the convergence of SGD. Some of which generally yield performance gains, whilst other may yield performance gains for particular types of problems. Perhaps the simplest of which is SGD with momentum (SGDM), which as the name suggests adds some parameter to the update rule which somehow attempts to respect the current rate at which the algorithm is traversing parameter space; speeding past shallow dips and slowing down for deep, narrow ones. Much akin to the physical quantity from which it is named. The update scheme may then be summarized by the following equations
\begin{align}\label{eqn: SGDM}
    \begin{split}
        \Delta \pmb w^{(k)} &= p \Delta\pmb w^{(k-1)} - \eta \nabla_wC(\pmb Y, \tilde{\pmb Y}(\pmb w)) \\
        \pmb w^{(k+1)} &= \pmb w^{(k)} + \Delta\pmb w^{(k)}
    \end{split}
\end{align}
where the momentum $p$ is usually set in the interval $[0, 1]$. Whilst simple, this scheme often yields significant performance gains over the standard constant learning rate methods, which in practice is rarely used \cite{Mehta_2019}. There also exists a plethora of other sophisticated schemes that in some way aims to increase the convergence rate of SGD, like the adaptive gradient (AdaGRAD), in which the learning rate is down-scaled by a cumulative history of the magnitude gradient. There is also the Root mean squared propagation (RMSProp) which works in a similar way to AdaGRAD by adapting the learning-rate to the gradient, but has an additional hyper-parameter which lets it "forget" older gradients. Common to many of these schemes however; is that they are often domain specific, and while they may yield exceptional performance for certain problems, there is no universal best choice.

\begin{figure}[h!tb]
    \center
    \vspace{5mm} % To avoid touching the preceding text
    \input{../figs/NN_fig.tex}
    \caption{\label{fig: NN Fig} Visual representation of a simple neural network with $2$ inputs, $2$ hidden layers with $3$, $2$ neurons respectively and an output layer with a single neuron.}
\end{figure}

\subsection{Neural Networks}
\noindent
Neural Networks are a class of algorithms in which a set of nodes, often referred to as Neurons, are connected as a weighted graph. Which as the name may suggests, aims to emulate behaviour similar to that of the human brain. Each of the nodes in this graph is then activated by an activation function, which takes the weighted input of all of its connected graphs like
\begin{equation}
    a = \sigma\qty(w_1 a_1' + w_2 a_2' + \dots + w_n a_n' + b)
\end{equation}
where $w_i$, $a_i'$ denotes the weights and activations of the connected nodes, and $\sigma$ the activation function of the node, which is chosen differently depending on the problem at hand. Lastly, we have also have a bias, $b$, which simply shifts the activation of the Node in the case of a binary activation, or tn the case where the activation function is chosen to be the identity function, the bias is simply the intercept.

The simplest type of neural network is the \textit{Feed-Forward Neural Network} (FFNN), which is a directed graph, consisting of layers where every neuron in the preceding layer is connected to every neuron in the following layer as depicted in Fig.~\ref{fig: NN Fig}, which is directed going from left to right.

As the name may suggest, the input layer is where the input data starts out, before being fed forward through the hidden layers of the network, weighted in-between each node before reaching the output layer, the final model. In a similar fashion to other methods of supervised learning, the neural network must first undergo a process of training to obtain a suitable set of weights and biases. This is done by optimizing the all of the weights and biases in the network wrt to a chosen cost function usually by SGD. However, due to the costly nature of evaluating all of these cost functions; an algorithm which cleverly exploits the chain rule of differentiation has been developed to very efficiently compute all of the gradients in the network in a process which is called backpropagation.Neural Networks and Deep Learning

\subsubsection{Backpropagation}
\noindent
We base our explanation of the backpropagation algorithm following more or less directly from \textcite{Nielsen} and \textcite{Mehta_2019}, referring the reader to those texts for the finer details and derivations of how the algorithm actually works, focusing instead on the computational aspects in the present texts. However, in short; the algorithm may be summarized as a clever use of the chain rule of differentiation.

Before we start, we first define a few quantities central to the network. First, we have the set of weights connecting each node in the network
\begin{equation}
    \{w^{1}_{jk}, \dots, w_{jk}^L\}
\end{equation}
where in a somewhat sloppy notation the $j, k$ indices denote the number of neurons in the current and previous layers respectively, thus spanning a different range for each $l$. We also define the set of inputs
\begin{equation}
    \{z_j^1, \dots, z_j^L\}
\end{equation}
which again adheres to the same convention of $j$ denoting the number of neurons in the current layer. These inputs then gets fed into the activation function $\sigma(z_j^l)$ of their corresponding neurons yielding the set of activations
\begin{equation}
    \{a_j^1, \dots, a_j^L\}
\end{equation}
where notably the activation function used in the output layer may differ from the one used in the hidden layers. We denote this special, output activation function as $\tilde \sigma(z_j^L)$.

We then start the backpropagation algorithm by first computing the response of each of the neurons in the network by the Feeding the input data forward in the network, starting with the first hidden layer, which is activated directly by the input data $X_p$ like
\begin{align}\label{eqn: FeedForward Initial}
    \begin{split}
        z^1_j &= w^1_{jk}X_k + b^1_j \\
        a^1_j &= \sigma(z^1_j)
    \end{split}
\end{align}
where we adopt the Einstein summation convention by summing over repeated indices.
We then similarly compute
\begin{align}
    \begin{split}
        z^l_{j} &= w^l_{jk}a^{l-1}_k + b^l_j \\
        a^l_{j} &= \sigma(z^l_{j})
    \end{split}
\end{align}
for hidden layers $l = 2, \dots, L$. Then, for the output layer we compute
\begin{align}
    \begin{split}
        z^{L}_j &= w^{L}_{jk}a^{L-1}_k + b^{L}_j \\
        a^{L}_j &= \tilde\sigma(z^{L}_j)
    \end{split}
\end{align}
where $a^{L}_j$ is the predicted response, and $\tilde\sigma$ is the activation function for the output layer, which as mentioned may differ from the activation function used in the hidden layers.

We then compute the error of the output as
\begin{equation}\label{eqn: delta L}
    \delta^{L}_j = \pdv{C}{a^{L}_{j}} \odot \tilde\sigma'(z^{L}_j)
\end{equation}
where $\odot$ denotes the Hadamard product, which is a form of element-wise multiplication of vectors and matrices. It is also worth to note that Eqn.~\ref{eqn: delta L} implicitly assumes that the derivatives of the output activation function $\pdv{z_j}\sigma(z^l_i) = 0 \enspace \forall \enspace i\neq j$. Whilst this holds true for a wide variety of activation functions; it does not hold for the SoftMax, which we will return to later on when we look at classification.

We then continue on by backpropagating the error like
\begin{equation}
    \delta^{l}_j = \qty(\delta^{l+1}_{k}\qty(w^{l+1})^T_{kj}) \odot \sigma'(z^l_j)
\end{equation}
for all $l = L-1, \dots, 1$.

We may then easily compute the gradients of the cost function wrt. the weights \& biases as

\begin{align}
    \begin{split}
        \pdv{C}{w_{jk}^l} &= \delta_j^l a^{l-1}_k \\
        \pdv{C}{b^l_j} &= \delta_j^l
    \end{split}
\end{align}
which are then used to update the weights and biases via gradient descent. Whilst this is in principle all there is to backpropagation, the way in which this method is performed requires some further modifications when solved on a computer.

\subsubsection{Backpropagation with mini-batches}
\noindent
In order to efficiently perform backpropagation simultaneously across several inputs at once, we need to slightly adjust our algorithm such that we may take advantage of the fast and efficient linear algebra libraries that are available, like in for example Numpy.

We then structure our input and output matrices adhering to the row-major storage of Numpy arrays by letting $X\in[M\times P], \enspace Y\in[M\times Q]$ where $M$ denotes the number of data points in the mini-batch and $P, Q$ the dimensionality of the input and output respectively. Explicitly, our data then undergoes the structure change

\begin{equation}
    X = \qty[
    \begin{matrix}
        X_1 \\ \vdots \\ X_P
    \end{matrix}
    ] \rightarrow
    X = \qty[
    \begin{matrix}
        X_{11} & \dots & X_{1P} \\
                 & \vdots&          \\
        X_{M1} & \dots & X_{MP}
    \end{matrix}
    ]
\end{equation}
similarly, we let $z^l_j \rightarrow z^l_{mj}$ and $a^l_{j}\rightarrow a^l_{mj}$ such that they are in accordance with $X$ and $Y$.

In order to adhere to this new form, we transpose Eqn.~\ref{eqn: FeedForward Initial} which yields
\begin{equation}
    w_{jk}X_K \rightarrow \qty(w_{jk}X_k)^T = X_k^T \qty(w^1)^T_{kj}
\end{equation}
Thus, we may write the initial step as
\begin{align}
    \begin{split}
        z^1_{mj} &= X_{mk}\qty(w^{1})^T_{kj} + b^1_j \\
        a^1_{mj} &= \sigma\qty(z^1_{mj})
    \end{split}
\end{align}
where the transposed bias is implicitly\footnote{Matching the behaviour of Numpys addition operator} added element-wise to each row in the resultant matrix. In a similar fashion, we feed forward for $l = 2, \dots L-1$, the last hidden layer as
\begin{align}
    \begin{split}
        z^l_{mj} &= a^{l-1}_{mk}\qty(w^l)^T_{kj} + b^L_j \\
        a^l_{mj} &= \sigma(z^l_{mj})
    \end{split}
\end{align}
and for the output layer
\begin{align}
    \begin{split}
        z^L_{mj} &= a^{L-1}_{mk}\qty(w^L)^T_{kj} + b^L_j \\
        a^L_{mj} &= \tilde\sigma\qty(z^L_{mj})
    \end{split}
\end{align}
Then, we compute the error of the output error as
\begin{equation}
    \delta^L_{mj} = \pdv{C}{a^L_{mj}} \odot \tilde\sigma'(z^L_{mj})
\end{equation}
which we then backpropagate throughout the layers for $l = L-1, \dots 1$
\begin{equation}
    \delta^{l}_{mj} = \delta^{l+1}_{mk}w^{l+1}_{kj} \odot \sigma'(z^l_{mj})
\end{equation}
and finally for the output layer. We the compute the derivatives of the cost functions wrt. the weights and biases for the input layer
\begin{align}
    \begin{split}
        \pdv{C}{w^{1}_{jk}} &= \qty(\delta^1)^T_{jm} X_{mk} \\
        \pdv{C}{b^1_{j}} &= \sum_m \delta^1_{mj}
    \end{split}
\end{align}
and similarly for layers $l = 2, \dots, L$ as
\begin{align}
    \begin{split}
        \pdv{C}{w^{l}_{jk}} &= \qty(\delta^1)^T_{jm} a^l_{mk} \\
        \pdv{C}{b^l_{j}} &= \sum_m \delta_{mj}
    \end{split}
\end{align}
The derivatives are then used to update the weights and biases by gradient descent in the same way as before. One may also following the same logic optimize for column-major storage, as is used in languages like Fortran, MATLAB, Julia, etc.

\subsubsection{Weight Initialization}
\noindent
For neural networks, particularly larger ones, the way in which the weights may contribute significantly to the convergence of the Network. Where in particular the somewhat naive approach by sampling all the weights in a constant fashion from some normal, or uniform distribution may lead to poor convergence for deeper networks; particularly with a variable number of nodes. Where by virtue of the number of nodes; the product of the weights in one layer may blow up, which in turn will blow up the response of the Network. Several clever schemes are thus employed too initialize the weights in a clever way, which are often specialized to some particular type of Neural Network. One such method is the Xavier initialization, proposed by \textcite{xavier}. In this method, the initial weights are sampled individually for each layer from a uniform distribution\footnote{As a sidenote, some sources suggest it as being sampled from a Gaussian instead. As is seemingly quite common in the ML literature there seemingly no standard practice with a lot of competing conventions. We however, opt to follow the original paper.} bounded by $\pm \sqrt{6/(j + k)}$, where $j, k$ denotes the number of neurons in the current and previous layers respectively. Thus, the magnitude of the initial weights are scaled down with respect to the size of $w^l_{jk}$, thus avoiding the aforementioned problem. Another type of initialization proposed by \textcite{he2015delving} is observed to perform particularly well of a ReLU type activation functions, which we will return to later. In He's initialization method, the weights are sampled from a standard normal distribution $\mathcal N(0, 1)$ scaled down by a factor $\sqrt{2/k}$, where $k$ denotes the number of neurons in the previous layer.

\subsection{Neural Network applications}
\subsubsection{Regression}

\subsubsection{Classification}

\begin{table*}[]
\caption{\label{tab: activation functions}Various Activation functions used in Neural Networks}
\setlength{\tabcolsep}{20pt}
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{llll}
    Name & Activation Function & Derivative & Range \\
    \hline\hline
    Identity &
    $\sigma(x) = x$  &
    $\sigma'(x) = 1$ &
    $(-\infty, \infty)$
    \\ \hline
    Sigmoid &
    $\sigma(x) = \frac{1}{1 + e^{-x}}$  &
    $\sigma'(x) =\sigma(x)\qty(1-\sigma(x))$ &
    $(0, 1)$
    \\ \hline
    Tanh &
    $\sigma(x) = \tanh(x)$  &
    $\sigma'(x) =  1 - \tanh^2(x)$ &
    $(0, 1)$
    \\ \hline
    ReLU    &
    $\sigma(x) = \left\{\begin{matrix}0 & \text{for } x \leq 0 \\ x & \text{for } 0 < x\end{matrix}\right.$ &
    $\sigma'(x) = \left\{\begin{matrix}0 & \text{for } x \leq 0 \\ 1 & \text{for } 0 < x\end{matrix}\right.$ &
    $[0, \infty)$
    \\ \hline
    LeakyReLU &
    $\sigma(x) = \left\{\begin{matrix}0.01x & \text{for } x \leq 0 \\ x & \text{for } 0 < x\end{matrix}\right.$ &
    $\sigma'(x) = \left\{\begin{matrix}0.01 & \text{for } x \leq 0 \\ 1 & \text{for } 0 < x\end{matrix}\right.$ &
    $(-\infty, \infty)$
    \\ \hline
\end{tabular}
\end{table*}



\subsection{Logistic Regression}
\noindent
As opposed to linear regression where the aim is to project data-points into some linear basis as to model some continuous function, logistic regression instead aims to project data unto some binary response. This may in turn be used classify whether or not some particular data point fits into some category. Thus, we wish to fit our data unto some function $f : \mathbb R \rightarrow [0, 1]$.
\begin{figure}[h!tb]
    \center
    \input{../figs/logistic.tex}
    \caption{The logistic function}
\end{figure}

\section{Results \& Discussion}

\begin{figure*}[h!tb]
    \center
    \subfloat{
    \includegraphics[width=\columnwidth]{SGD_learning_rate.pdf}
    }
    \subfloat{
    \includegraphics[width=\columnwidth]{SGD_learning_penalty.pdf}
    }
\end{figure*}

\section{Conclusion}

\onecolumngrid
\bibliography{bibfile}
\newpage
\twocolumngrid
\appendix
\end{document}
